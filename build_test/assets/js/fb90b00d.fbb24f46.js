"use strict";(globalThis.webpackChunkq4_hackathon=globalThis.webpackChunkq4_hackathon||[]).push([[3754],{8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>s});var o=t(6540);const r={},a=o.createContext(r);function i(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:i(n.components),o.createElement(a.Provider,{value:e},n.children)}},8584:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module3-nvidia-isaac/lesson3-isaac-robot-control","title":"Lesson 3 - Robot Control and Navigation with Isaac","description":"Overview","source":"@site/docs/module3-nvidia-isaac/lesson3-isaac-robot-control.md","sourceDirName":"module3-nvidia-isaac","slug":"/module3-nvidia-isaac/lesson3-isaac-robot-control","permalink":"/Q4-hackathon1/docs/module3-nvidia-isaac/lesson3-isaac-robot-control","draft":false,"unlisted":false,"editUrl":"https://github.com/iffatmumtaz/Q4-hackathon1/edit/main/docs/module3-nvidia-isaac/lesson3-isaac-robot-control.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Isaac Sim Robot Control","title":"Lesson 3 - Robot Control and Navigation with Isaac"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Perception Pipelines","permalink":"/Q4-hackathon1/docs/module3-nvidia-isaac/lesson2-isaac-perception-pipelines"},"next":{"title":"LLM-Robot Interface","permalink":"/Q4-hackathon1/docs/module4-vla/lesson1-llm-robot-interface"}}');var r=t(4848),a=t(8453);const i={sidebar_label:"Isaac Sim Robot Control",title:"Lesson 3 - Robot Control and Navigation with Isaac"},s="Lesson 3: Robot Control and Navigation with Isaac Sim",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Goals",id:"learning-goals",level:2},{value:"Concepts",id:"concepts",level:2},{value:"Robot Control Fundamentals",id:"robot-control-fundamentals",level:3},{value:"Navigation in Robotics",id:"navigation-in-robotics",level:3},{value:"Isaac Sim Control Architecture:",id:"isaac-sim-control-architecture",level:3},{value:"Steps",id:"steps",level:2},{value:"Step 1: Understanding Isaac Sim Control Interfaces",id:"step-1-understanding-isaac-sim-control-interfaces",level:3},{value:"Step 2: Implementing Differential Drive Control",id:"step-2-implementing-differential-drive-control",level:3},{value:"Step 3: Implementing Advanced Navigation with Perception Integration",id:"step-3-implementing-advanced-navigation-with-perception-integration",level:3},{value:"Step 4: ROS 2 Integration for Navigation",id:"step-4-ros-2-integration-for-navigation",level:3},{value:"Code",id:"code",level:2},{value:"Complete Isaac Sim Navigation System with Control and Perception:",id:"complete-isaac-sim-navigation-system-with-control-and-perception",level:3},{value:"Examples",id:"examples",level:2},{value:"Navigation in Different Scenarios:",id:"navigation-in-different-scenarios",level:3},{value:"Control Strategies:",id:"control-strategies",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Required Tools &amp; Software",id:"required-tools--software",level:2},{value:"Expected Outcome",id:"expected-outcome",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"lesson-3-robot-control-and-navigation-with-isaac-sim",children:"Lesson 3: Robot Control and Navigation with Isaac Sim"})}),"\n",(0,r.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(e.p,{children:"This lesson focuses on implementing robot control and navigation systems using NVIDIA Isaac Sim. You'll learn how to create control algorithms, implement navigation behaviors, and connect these systems to ROS 2 for integrated robotics workflows. The lesson emphasizes practical implementation of control systems for humanoid robotics applications."}),"\n",(0,r.jsx)(e.h2,{id:"learning-goals",children:"Learning Goals"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement robot control algorithms in Isaac Sim"}),"\n",(0,r.jsx)(e.li,{children:"Create navigation behaviors using path planning and obstacle avoidance"}),"\n",(0,r.jsx)(e.li,{children:"Integrate control systems with perception for closed-loop operation"}),"\n",(0,r.jsx)(e.li,{children:"Connect Isaac Sim control systems to ROS 2 navigation stack"}),"\n",(0,r.jsx)(e.li,{children:"Validate control performance in simulation before real-world deployment"}),"\n",(0,r.jsx)(e.li,{children:"Understand the principles of robot control for humanoid applications"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"concepts",children:"Concepts"}),"\n",(0,r.jsx)(e.h3,{id:"robot-control-fundamentals",children:"Robot Control Fundamentals"}),"\n",(0,r.jsx)(e.p,{children:"Robot control involves generating appropriate commands to achieve desired robot behavior. Key concepts include:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Open-Loop Control"}),": Commands sent without feedback"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Closed-Loop Control"}),": Commands adjusted based on sensor feedback"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"PID Control"}),": Proportional-Integral-Derivative control for precise positioning"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Trajectory Following"}),": Following predefined paths with smooth motion"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Impedance Control"}),": Controlling robot compliance and interaction forces"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"navigation-in-robotics",children:"Navigation in Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Navigation systems enable robots to move autonomously in environments:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Global Path Planning"}),": Finding paths from start to goal using known maps"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Local Path Planning"}),": Avoiding obstacles in real-time during navigation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Localization"}),": Determining robot position in the environment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Mapping"}),": Creating representations of the environment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"isaac-sim-control-architecture",children:"Isaac Sim Control Architecture:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Articulation Control"}),": Low-level joint control for articulated robots"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Differential Drive"}),": Control for wheeled robots"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Leg Control"}),": Specialized control for legged robots"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS Bridge"}),": Integration with ROS control and navigation stacks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AI Control"}),": Reinforcement learning and imitation learning for complex behaviors"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"steps",children:"Steps"}),"\n",(0,r.jsx)(e.h3,{id:"step-1-understanding-isaac-sim-control-interfaces",children:"Step 1: Understanding Isaac Sim Control Interfaces"}),"\n",(0,r.jsx)(e.p,{children:"Isaac Sim provides multiple ways to control robots:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# isaac_control_interfaces.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.utils.types import ArticulationAction\nimport numpy as np\n\n\nclass IsaacControlInterfaces:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.initialized = False\n\n        self._setup_scene()\n        self._initialize_robot()\n\n    def _setup_scene(self):\n        """Setup the scene with robot and environment"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add a simple wheeled robot\n        robot_path = assets_root_path + "/Isaac/Robots/Carter/carter.modelUSD.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Robot")\n\n        # Add ground plane and obstacles\n        self.world.scene.add_default_ground_plane()\n\n        from omni.isaac.core.objects import FixedCuboid\n        self.world.scene.add(\n            FixedCuboid(\n                prim_path="/World/Obstacle",\n                name="obstacle",\n                position=np.array([1.0, 0.0, 0.5]),\n                size=0.5,\n                color=np.array([0.8, 0.2, 0.2])\n            )\n        )\n\n        self.world.reset()\n\n    def _initialize_robot(self):\n        """Initialize robot control interface"""\n        try:\n            # Get robot as articulation\n            self.robot = self.world.scene.get_object("Robot")\n            if self.robot is None:\n                print("Could not find robot in scene")\n                return\n\n            # Initialize the articulation\n            self.robot.initialize(world_prim=self.world.scene)\n            self.initialized = True\n            print("Robot initialized successfully")\n        except Exception as e:\n            print(f"Error initializing robot: {e}")\n\n    def position_control(self, joint_positions):\n        """Control robot joints to reach desired positions"""\n        if not self.initialized or self.robot is None:\n            return False\n\n        try:\n            # Apply position commands to all joints\n            self.robot.apply_articulation_actions(\n                ArticulationAction(joint_positions=joint_positions)\n            )\n            return True\n        except Exception as e:\n            print(f"Error in position control: {e}")\n            return False\n\n    def velocity_control(self, joint_velocities):\n        """Control robot joints with desired velocities"""\n        if not self.initialized or self.robot is None:\n            return False\n\n        try:\n            # Apply velocity commands\n            self.robot.apply_articulation_actions(\n                ArticulationAction(joint_velocities=joint_velocities)\n            )\n            return True\n        except Exception as e:\n            print(f"Error in velocity control: {e}")\n            return False\n\n    def effort_control(self, joint_efforts):\n        """Control robot joints with desired efforts/torques"""\n        if not self.initialized or self.robot is None:\n            return False\n\n        try:\n            # Apply effort commands\n            self.robot.apply_articulation_actions(\n                ArticulationAction(joint_efforts=joint_efforts)\n            )\n            return True\n        except Exception as e:\n            print(f"Error in effort control: {e}")\n            return False\n\n    def get_robot_state(self):\n        """Get current robot state"""\n        if not self.initialized or self.robot is None:\n            return None\n\n        try:\n            # Get joint states\n            joint_positions = self.robot.get_joint_positions()\n            joint_velocities = self.robot.get_joint_velocities()\n            joint_efforts = self.robot.get_joint_efforts()\n\n            # Get base pose\n            world_pos, world_ori = self.robot.get_world_pose()\n\n            return {\n                \'positions\': joint_positions,\n                \'velocities\': joint_velocities,\n                \'efforts\': joint_efforts,\n                \'world_position\': world_pos,\n                \'world_orientation\': world_ori\n            }\n        except Exception as e:\n            print(f"Error getting robot state: {e}")\n            return None\n\n\ndef main():\n    control_system = IsaacControlInterfaces()\n\n    # Example: Move robot joints to zero position\n    if control_system.initialized:\n        zero_positions = [0.0] * control_system.robot.num_dof\n        control_system.position_control(zero_positions)\n\n        # Run simulation to see the effect\n        for i in range(100):\n            control_system.world.step(render=True)\n\n    print("Control interface example completed")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"step-2-implementing-differential-drive-control",children:"Step 2: Implementing Differential Drive Control"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# differential_drive_control.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.wheeled_robots.controllers.differential_controller import DifferentialController\nfrom omni.isaac.wheeled_robots.robots import WheeledRobot\nimport numpy as np\n\n\nclass DifferentialDriveController:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.controller = None\n        self.initialized = False\n\n        self._setup_scene()\n        self._initialize_robot()\n\n    def _setup_scene(self):\n        """Setup scene with wheeled robot"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add a wheeled robot (Carter)\n        robot_path = assets_root_path + "/Isaac/Robots/Carter/carter.modelUSD.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Carter")\n\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n        # Add some obstacles\n        from omni.isaac.core.objects import FixedCuboid\n        for i in range(3):\n            self.world.scene.add(\n                FixedCuboid(\n                    prim_path=f"/World/Obstacle{i}",\n                    name=f"obstacle{i}",\n                    position=np.array([i*2.0, 1.0, 0.5]),\n                    size=0.5,\n                    color=np.array([0.8, 0.2, 0.2])\n                )\n            )\n\n        self.world.reset()\n\n    def _initialize_robot(self):\n        """Initialize the wheeled robot and controller"""\n        try:\n            # Create wheeled robot object\n            self.robot = WheeledRobot(\n                prim_path="/World/Carter",\n                name="carter",\n                wheel_dof_names=["left_wheel_joint", "right_wheel_joint"],\n                create_robot=True\n            )\n\n            # Add to world\n            self.world.scene.add(self.robot)\n\n            # Initialize controller with parameters\n            self.controller = DifferentialController(\n                name="simple_control",\n                wheel_radius=0.1,  # Carter wheel radius\n                wheel_base=0.4,    # Distance between wheels\n                max_linear_velocity=1.0,\n                max_angular_velocity=np.pi\n            )\n\n            # Initialize the world\n            self.world.reset()\n            self.initialized = True\n            print("Differential drive robot initialized successfully")\n        except Exception as e:\n            print(f"Error initializing robot: {e}")\n\n    def move_robot(self, linear_velocity, angular_velocity, duration=1.0):\n        """Move robot with specified linear and angular velocities"""\n        if not self.initialized:\n            return\n\n        # Calculate number of steps for duration\n        steps = int(duration / self.world.get_physics_dt())\n\n        for _ in range(steps):\n            # Get current robot state\n            current_action = self.controller.forward(\n                current_joint_velocities=self.robot.get_joints_state().joint_velocities,\n                current_joint_positions=self.robot.get_joints_state().joint_positions,\n                joint_efforts=None,\n                target_linear_velocity=linear_velocity,\n                target_angular_velocity=angular_velocity\n            )\n\n            # Apply action to robot\n            self.robot.apply_action(current_action)\n\n            # Step the world\n            self.world.step(render=True)\n\n    def navigate_to_waypoint(self, target_x, target_y):\n        """Navigate robot to a specific waypoint using simple control"""\n        if not self.initialized:\n            return\n\n        # Get current robot position\n        current_pos, _ = self.robot.get_world_pose()\n        current_x, current_y = current_pos[0], current_pos[1]\n\n        # Calculate desired direction\n        dx = target_x - current_x\n        dy = target_y - current_y\n        distance = np.sqrt(dx*dx + dy*dy)\n\n        if distance < 0.1:  # Already at target\n            print("Already at target position")\n            return\n\n        # Calculate target angle\n        target_angle = np.arctan2(dy, dx)\n\n        # Get current orientation\n        _, current_rot = self.robot.get_world_pose()\n        current_angle = np.arctan2(\n            2 * (current_rot[3] * current_rot[2] + current_rot[0] * current_rot[1]),\n            1 - 2 * (current_rot[1]**2 + current_rot[2]**2)\n        )\n\n        # Calculate angular error\n        angle_error = target_angle - current_angle\n        # Normalize angle to [-pi, pi]\n        angle_error = np.arctan2(np.sin(angle_error), np.cos(angle_error))\n\n        # Simple proportional control\n        linear_vel = min(0.5, distance)  # Slow down as we approach\n        angular_vel = 2.0 * angle_error  # Proportional control for orientation\n\n        print(f"Moving to ({target_x}, {target_y}), distance: {distance:.2f}, angle_error: {angle_error:.2f}")\n\n        # Move robot\n        self.move_robot(linear_vel, angular_vel, duration=0.5)\n\n\ndef main():\n    nav_controller = DifferentialDriveController()\n\n    if nav_controller.initialized:\n        # Navigate to a waypoint\n        nav_controller.navigate_to_waypoint(2.0, 1.0)\n\n        # Move forward a bit more\n        nav_controller.move_robot(linear_velocity=0.5, angular_velocity=0.0, duration=2.0)\n\n    print("Differential drive control example completed")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"step-3-implementing-advanced-navigation-with-perception-integration",children:"Step 3: Implementing Advanced Navigation with Perception Integration"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# perception_control_integration.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.wheeled_robots.robots import WheeledRobot\nfrom omni.isaac.wheeled_robots.controllers.differential_controller import DifferentialController\nimport numpy as np\nimport cv2\n\n\nclass PerceptionControlIntegration:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.controller = None\n        self.camera = None\n        self.initialized = False\n\n        self._setup_scene()\n        self._initialize_system()\n\n    def _setup_scene(self):\n        """Setup scene with robot, camera, and environment"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add robot\n        robot_path = assets_root_path + "/Isaac/Robots/Carter/carter.modelUSD.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Carter")\n\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n        # Add objects to detect\n        from omni.isaac.core.objects import DynamicCuboid\n        self.world.scene.add(\n            DynamicCuboid(\n                prim_path="/World/Target",\n                name="target",\n                position=np.array([2.0, 0.0, 0.5]),\n                size=0.3,\n                color=np.array([0.0, 1.0, 0.0])  # Green target\n            )\n        )\n\n        self.world.reset()\n\n    def _initialize_system(self):\n        """Initialize robot and camera"""\n        try:\n            # Initialize robot\n            self.robot = WheeledRobot(\n                prim_path="/World/Carter",\n                name="carter",\n                wheel_dof_names=["left_wheel_joint", "right_wheel_joint"],\n                create_robot=True\n            )\n            self.world.scene.add(self.robot)\n\n            # Initialize controller\n            self.controller = DifferentialController(\n                name="nav_control",\n                wheel_radius=0.1,\n                wheel_base=0.4,\n                max_linear_velocity=1.0,\n                max_angular_velocity=np.pi\n            )\n\n            # Add camera to robot\n            self.camera = Camera(\n                prim_path="/World/Carter/Looks/visual/camera",\n                frequency=30,\n                resolution=(320, 240)\n            )\n\n            # If camera path doesn\'t work, try alternative\n            if not self.camera.is_valid():\n                self.camera = Camera(\n                    prim_path="/World/Carter/camera",\n                    frequency=30,\n                    resolution=(320, 240)\n                )\n                self.camera.set_translation(np.array([0.2, 0.0, 0.1]))\n\n            # Reset world\n            self.world.reset()\n            self.initialized = True\n            print("Perception-control system initialized successfully")\n        except Exception as e:\n            print(f"Error initializing system: {e}")\n\n    def detect_target_in_image(self, image):\n        """Detect green target in image using color filtering"""\n        if image is None:\n            return None\n\n        # Convert to HSV for color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Define range for green color\n        lower_green = np.array([40, 50, 50])\n        upper_green = np.array([80, 255, 255])\n        mask = cv2.inRange(hsv, lower_green, upper_green)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Get the largest contour\n            largest_contour = max(contours, key=cv2.contourArea)\n            if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(largest_contour)\n                center_x = x + w/2\n                center_y = y + h/2\n                image_width = image.shape[1]\n\n                # Calculate horizontal offset (-1 to 1) where 0 is center\n                offset = (center_x - image_width/2) / (image_width/2)\n\n                return {\n                    \'offset\': offset,\n                    \'area\': cv2.contourArea(largest_contour),\n                    \'bbox\': (x, y, w, h)\n                }\n\n        return None\n\n    def navigate_with_perception(self, max_steps=500):\n        """Navigate robot using perception feedback"""\n        if not self.initialized:\n            return\n\n        print("Starting perception-based navigation...")\n\n        for step in range(max_steps):\n            # Step the world\n            self.world.step(render=True)\n\n            # Get camera image\n            rgb_image = self.camera.get_rgb()\n\n            if rgb_image is not None:\n                # Detect target in image\n                target_info = self.detect_target_in_image(rgb_image)\n\n                if target_info:\n                    # Target detected - navigate towards it\n                    offset = target_info[\'offset\']\n                    area = target_info[\'area\']\n\n                    # Calculate control based on target position\n                    linear_vel = 0.5  # Move forward\n                    angular_vel = -1.5 * offset  # Turn towards target\n\n                    # Slow down as we get closer (based on target size)\n                    if area > 5000:  # Target is large (close)\n                        linear_vel = 0.2\n\n                    print(f"Step {step}: Target detected, offset: {offset:.2f}, area: {area:.0f}")\n                else:\n                    # No target detected - search for it\n                    linear_vel = 0.0\n                    angular_vel = 0.5  # Turn in place to search\n\n                    print(f"Step {step}: Searching for target...")\n            else:\n                # No image available - use default search behavior\n                linear_vel = 0.0\n                angular_vel = 0.5\n\n            # Apply control commands\n            if self.robot and self.controller:\n                try:\n                    current_action = self.controller.forward(\n                        current_joint_velocities=self.robot.get_joints_state().joint_velocities,\n                        current_joint_positions=self.robot.get_joints_state().joint_positions,\n                        joint_efforts=None,\n                        target_linear_velocity=linear_vel,\n                        target_angular_velocity=angular_vel\n                    )\n                    self.robot.apply_action(current_action)\n                except:\n                    # If controller fails, try direct velocity control\n                    pass\n\n            # Check if we\'re close to the target\n            robot_pos, _ = self.robot.get_world_pose()\n            target_pos = np.array([2.0, 0.0, 0.5])\n            distance = np.linalg.norm(robot_pos[:2] - target_pos[:2])\n\n            if distance < 0.3:\n                print(f"Reached target! Distance: {distance:.2f}")\n                break\n\n        print("Navigation completed")\n\n\ndef main():\n    perception_nav = PerceptionControlIntegration()\n\n    if perception_nav.initialized:\n        perception_nav.navigate_with_perception(max_steps=1000)\n\n    print("Perception-control integration example completed")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"step-4-ros-2-integration-for-navigation",children:"Step 4: ROS 2 Integration for Navigation"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# isaac_navigation_ros.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped, Point\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import String\nfrom tf2_ros import TransformBroadcaster\nimport tf2_ros\nimport tf2_geometry_msgs\nfrom geometry_msgs.msg import TransformStamped\nimport numpy as np\nimport math\n\n\nclass IsaacNavigationBridge(Node):\n    """Bridge between Isaac Sim navigation and ROS 2"""\n\n    def __init__(self):\n        super().__init__(\'isaac_navigation_bridge\')\n\n        # Publishers for navigation data\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'/odom\', 10)\n        self.laser_pub = self.create_publisher(LaserScan, \'/scan\', 10)\n\n        # Subscribers for navigation commands\n        self.nav_goal_sub = self.create_subscription(\n            PoseStamped, \'/move_base_simple/goal\', self.nav_goal_callback, 10)\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel_input\', self.cmd_vel_input_callback, 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Timer for publishing simulated data\n        self.nav_timer = self.create_timer(0.1, self.publish_navigation_data)\n\n        # Robot state tracking\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0\n        self.linear_vel = 0.0\n        self.angular_vel = 0.0\n\n        self.get_logger().info(\'Isaac Navigation Bridge initialized\')\n\n    def nav_goal_callback(self, msg):\n        """Handle navigation goal from ROS"""\n        target_x = msg.pose.position.x\n        target_y = msg.pose.position.y\n\n        self.get_logger().info(f\'Received navigation goal: ({target_x}, {target_y})\')\n\n        # In a real implementation, this would send the goal to Isaac Sim\n        # For this example, we\'ll just store it\n        self.navigate_to_goal(target_x, target_y)\n\n    def cmd_vel_input_callback(self, msg):\n        """Handle velocity commands from ROS"""\n        self.linear_vel = msg.linear.x\n        self.angular_vel = msg.angular.z\n\n        self.get_logger().debug(f\'Received cmd_vel: linear={self.linear_vel}, angular={self.angular_vel}\')\n\n    def navigate_to_goal(self, target_x, target_y):\n        """Navigate to specified goal coordinates"""\n        # Calculate direction to goal\n        dx = target_x - self.robot_x\n        dy = target_y - self.robot_y\n        distance = math.sqrt(dx*dx + dy*dy)\n\n        if distance > 0.1:  # If not already at goal\n            target_angle = math.atan2(dy, dx)\n\n            # Calculate angular error\n            angle_error = target_angle - self.robot_theta\n            # Normalize angle to [-pi, pi]\n            angle_error = math.atan2(math.sin(angle_error), math.cos(angle_error))\n\n            # Simple proportional control\n            self.linear_vel = min(0.5, distance)  # Move faster when farther away\n            self.angular_vel = 2.0 * angle_error   # Turn toward goal\n\n    def publish_navigation_data(self):\n        """Publish navigation-related data to ROS"""\n        # Update robot position based on current velocities\n        dt = 0.1  # Time step from timer\n        self.robot_x += self.linear_vel * math.cos(self.robot_theta) * dt\n        self.robot_y += self.linear_vel * math.sin(self.robot_theta) * dt\n        self.robot_theta += self.angular_vel * dt\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'odom\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Position\n        odom_msg.pose.pose.position.x = self.robot_x\n        odom_msg.pose.pose.position.y = self.robot_y\n        odom_msg.pose.pose.position.z = 0.0\n\n        # Orientation (convert theta to quaternion)\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, self.robot_theta)\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        # Velocity\n        odom_msg.twist.twist.linear.x = self.linear_vel\n        odom_msg.twist.twist.angular.z = self.angular_vel\n\n        self.odom_pub.publish(odom_msg)\n\n        # Publish TF\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n        t.transform.translation.x = self.robot_x\n        t.transform.translation.y = self.robot_y\n        t.transform.translation.z = 0.0\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n\n        self.tf_broadcaster.sendTransform(t)\n\n        # Publish laser scan (simulated)\n        laser_msg = self._generate_simulated_laser_scan()\n        laser_msg.header.stamp = self.get_clock().now().to_msg()\n        laser_msg.header.frame_id = \'laser_frame\'\n        self.laser_pub.publish(laser_msg)\n\n    def _generate_simulated_laser_scan(self):\n        """Generate simulated laser scan data"""\n        laser_msg = LaserScan()\n        laser_msg.angle_min = -math.pi / 2\n        laser_msg.angle_max = math.pi / 2\n        laser_msg.angle_increment = math.pi / 180  # 1 degree\n        laser_msg.time_increment = 0.0\n        laser_msg.scan_time = 0.1\n        laser_msg.range_min = 0.1\n        laser_msg.range_max = 10.0\n\n        # Generate 181 readings for 180 degrees at 1 degree increments\n        num_readings = 181\n        laser_msg.ranges = [float(\'inf\')] * num_readings\n\n        # Add some simulated obstacles\n        for i in range(num_readings):\n            angle = laser_msg.angle_min + i * laser_msg.angle_increment\n\n            # Simulate a circular obstacle in front of robot\n            obstacle_distance = 2.0  # meters away\n            obstacle_size = 1.0      # meters in diameter\n\n            # If this ray hits the obstacle\n            if abs(angle) < math.asin(obstacle_size / (2 * obstacle_distance)):\n                laser_msg.ranges[i] = obstacle_distance\n\n        return laser_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_bridge = IsaacNavigationBridge()\n\n    try:\n        rclpy.spin(nav_bridge)\n    except KeyboardInterrupt:\n        nav_bridge.get_logger().info(\'Navigation bridge interrupted\')\n    finally:\n        nav_bridge.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h2,{id:"code",children:"Code"}),"\n",(0,r.jsx)(e.h3,{id:"complete-isaac-sim-navigation-system-with-control-and-perception",children:"Complete Isaac Sim Navigation System with Control and Perception:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# complete_navigation_system.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera, LidarRtx\nfrom omni.isaac.wheeled_robots.robots import WheeledRobot\nfrom omni.isaac.wheeled_robots.controllers.differential_controller import DifferentialController\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport cv2\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import LaserScan, Image\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport threading\nimport time\n\n\nclass IsaacNavigationSystem(Node):\n    """Complete navigation system combining Isaac Sim with ROS 2"""\n\n    def __init__(self):\n        super().__init__(\'isaac_navigation_system\')\n\n        # ROS components\n        self.bridge = CvBridge()\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'/odom\', 10)\n        self.camera_pub = self.create_publisher(Image, \'/camera/rgb\', 10)\n        self.laser_pub = self.create_publisher(LaserScan, \'/scan\', 10)\n\n        # Subscribers\n        self.nav_goal_sub = self.create_subscription(\n            PoseStamped, \'/move_base_simple/goal\', self.nav_goal_callback, 10)\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel_input\', self.cmd_vel_input_callback, 10)\n\n        # Isaac Sim components\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.controller = None\n        self.camera = None\n        self.lidar = None\n        self.synthetic_data_helper = None\n\n        # Robot state\n        self.linear_vel = 0.0\n        self.angular_vel = 0.0\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0\n\n        # Setup Isaac Sim\n        self._setup_isaac_sim()\n\n        # Navigation thread\n        self.nav_thread = threading.Thread(target=self._navigation_loop, daemon=True)\n        self.nav_thread.start()\n\n        self.get_logger().info(\'Complete Isaac Navigation System initialized\')\n\n    def _setup_isaac_sim(self):\n        """Setup Isaac Sim environment and sensors"""\n        try:\n            # Setup scene\n            assets_root_path = get_assets_root_path()\n            if assets_root_path is None:\n                self.get_logger().error("Could not find Isaac Sim assets")\n                return\n\n            # Add robot\n            robot_path = assets_root_path + "/Isaac/Robots/Carter/carter.modelUSD.usd"\n            add_reference_to_stage(usd_path=robot_path, prim_path="/World/Carter")\n\n            # Add ground plane\n            self.world.scene.add_default_ground_plane()\n\n            # Add obstacles\n            from omni.isaac.core.objects import FixedCuboid\n            for i in range(5):\n                self.world.scene.add(\n                    FixedCuboid(\n                        prim_path=f"/World/Obstacle{i}",\n                        name=f"obstacle{i}",\n                        position=np.array([i*1.5 - 3.0, 2.0, 0.5]),\n                        size=0.5,\n                        color=np.array([0.8, 0.2, 0.2])\n                    )\n                )\n\n            # Initialize world\n            self.world.reset()\n\n            # Initialize robot\n            self.robot = WheeledRobot(\n                prim_path="/World/Carter",\n                name="carter",\n                wheel_dof_names=["left_wheel_joint", "right_wheel_joint"],\n                create_robot=True\n            )\n            self.world.scene.add(self.robot)\n\n            # Initialize controller\n            self.controller = DifferentialController(\n                name="complete_control",\n                wheel_radius=0.1,\n                wheel_base=0.4,\n                max_linear_velocity=1.0,\n                max_angular_velocity=np.pi\n            )\n\n            # Add sensors\n            self.camera = Camera(\n                prim_path="/World/Carter/camera",\n                frequency=30,\n                resolution=(640, 480)\n            )\n            self.camera.set_translation(np.array([0.2, 0.0, 0.1]))\n\n            # Initialize synthetic data helper\n            self.synthetic_data_helper = SyntheticDataHelper()\n            self.synthetic_data_helper.enable_data_type("rgb", device="cpu")\n\n            self.get_logger().info("Isaac Sim navigation system setup completed")\n\n        except Exception as e:\n            self.get_logger().error(f"Error in Isaac Sim setup: {e}")\n\n    def nav_goal_callback(self, msg):\n        """Handle navigation goal from ROS"""\n        target_x = msg.pose.position.x\n        target_y = msg.pose.position.y\n        self.get_logger().info(f"Received navigation goal: ({target_x}, {target_y})")\n\n        # In a real implementation, this would initiate path planning\n        # For this example, we\'ll use simple proportional navigation\n        self.navigate_to_goal(target_x, target_y)\n\n    def cmd_vel_input_callback(self, msg):\n        """Handle velocity commands from ROS"""\n        self.linear_vel = msg.linear.x\n        self.angular_vel = msg.angular.z\n\n    def navigate_to_goal(self, target_x, target_y):\n        """Navigate to specified goal with obstacle avoidance"""\n        # Calculate direction to goal\n        dx = target_x - self.robot_x\n        dy = target_y - self.robot_y\n        distance = np.sqrt(dx*dx + dy*dy)\n\n        if distance > 0.2:  # If not already at goal\n            target_angle = np.arctan2(dy, dx)\n\n            # Calculate angular error\n            current_pos, current_rot = self.robot.get_world_pose()\n            current_angle = np.arctan2(\n                2 * (current_rot[3] * current_rot[2] + current_rot[0] * current_rot[1]),\n                1 - 2 * (current_rot[1]**2 + current_rot[2]**2)\n            )\n\n            angle_error = target_angle - current_angle\n            angle_error = np.arctan2(np.sin(angle_error), np.cos(angle_error))\n\n            # Simple navigation with obstacle avoidance\n            self.linear_vel = min(0.5, distance)  # Move faster when farther away\n            self.angular_vel = 2.0 * angle_error   # Turn toward goal\n\n    def _navigation_loop(self):\n        """Main navigation loop running in separate thread"""\n        rate = self.world.get_physics_dt()\n\n        while True:\n            try:\n                # Step Isaac Sim world\n                self.world.step(render=True)\n\n                # Update robot control based on current velocities\n                if self.robot and self.controller:\n                    try:\n                        current_action = self.controller.forward(\n                            current_joint_velocities=self.robot.get_joints_state().joint_velocities,\n                            current_joint_positions=self.robot.get_joints_state().joint_positions,\n                            joint_efforts=None,\n                            target_linear_velocity=self.linear_vel,\n                            target_angular_velocity=self.angular_vel\n                        )\n                        self.robot.apply_action(current_action)\n                    except Exception as e:\n                        self.get_logger().debug(f"Control error: {e}")\n\n                # Get sensor data\n                self._publish_sensor_data()\n\n                # Update robot state for ROS\n                self._update_robot_state()\n\n                time.sleep(rate)\n\n            except Exception as e:\n                self.get_logger().error(f"Error in navigation loop: {e}")\n                time.sleep(0.1)  # Brief pause on error\n\n    def _publish_sensor_data(self):\n        """Publish sensor data to ROS"""\n        # Publish camera image\n        try:\n            rgb_image = self.camera.get_rgb()\n            if rgb_image is not None:\n                ros_image = self.bridge.cv2_to_imgmsg(rgb_image, encoding=\'rgb8\')\n                ros_image.header.stamp = self.get_clock().now().to_msg()\n                ros_image.header.frame_id = \'camera_rgb_optical_frame\'\n                self.camera_pub.publish(ros_image)\n        except Exception as e:\n            self.get_logger().debug(f"Camera error: {e}")\n\n        # Publish simulated laser scan\n        laser_msg = self._generate_simulated_laser_scan()\n        laser_msg.header.stamp = self.get_clock().now().to_msg()\n        laser_msg.header.frame_id = \'laser_frame\'\n        self.laser_pub.publish(laser_msg)\n\n    def _update_robot_state(self):\n        """Update robot state for ROS publishing"""\n        try:\n            # Get robot pose from Isaac Sim\n            pos, rot = self.robot.get_world_pose()\n            self.robot_x, self.robot_y = pos[0], pos[1]\n\n            # Convert quaternion to euler for theta\n            self.robot_theta = np.arctan2(\n                2 * (rot[3] * rot[2] + rot[0] * rot[1]),\n                1 - 2 * (rot[1]**2 + rot[2]**2)\n            )\n\n            # Publish odometry\n            odom_msg = Odometry()\n            odom_msg.header.stamp = self.get_clock().now().to_msg()\n            odom_msg.header.frame_id = \'odom\'\n            odom_msg.child_frame_id = \'base_link\'\n\n            odom_msg.pose.pose.position.x = float(self.robot_x)\n            odom_msg.pose.pose.position.y = float(self.robot_y)\n            odom_msg.pose.pose.position.z = float(pos[2])\n\n            # Convert to quaternion\n            from tf_transformations import quaternion_from_euler\n            quat = quaternion_from_euler(0, 0, float(self.robot_theta))\n            odom_msg.pose.pose.orientation.x = quat[0]\n            odom_msg.pose.pose.orientation.y = quat[1]\n            odom_msg.pose.pose.orientation.z = quat[2]\n            odom_msg.pose.pose.orientation.w = quat[3]\n\n            # Velocity\n            odom_msg.twist.twist.linear.x = float(self.linear_vel)\n            odom_msg.twist.twist.angular.z = float(self.angular_vel)\n\n            self.odom_pub.publish(odom_msg)\n\n        except Exception as e:\n            self.get_logger().debug(f"State update error: {e}")\n\n    def _generate_simulated_laser_scan(self):\n        """Generate simulated laser scan with obstacle detection"""\n        laser_msg = LaserScan()\n        laser_msg.angle_min = -np.pi / 2\n        laser_msg.angle_max = np.pi / 2\n        laser_msg.angle_increment = np.pi / 180\n        laser_msg.time_increment = 0.0\n        laser_msg.scan_time = 0.1\n        laser_msg.range_min = 0.1\n        laser_msg.range_max = 10.0\n\n        num_readings = 181\n        laser_msg.ranges = [float(\'inf\')] * num_readings\n\n        # In a real implementation, this would use Isaac Sim\'s LIDAR\n        # For this example, we\'ll simulate based on robot position\n        robot_pos, _ = self.robot.get_world_pose() if self.robot else (np.array([0, 0, 0]), np.array([0, 0, 0, 1]))\n\n        for i in range(num_readings):\n            angle = laser_msg.angle_min + i * laser_msg.angle_increment\n            # Add simulated obstacles based on environment\n            if abs(angle) < 0.5:  # Forward direction\n                laser_msg.ranges[i] = 3.0  # Simulate obstacle 3m ahead\n\n        return laser_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_system = IsaacNavigationSystem()\n\n    try:\n        rclpy.spin(nav_system)\n    except KeyboardInterrupt:\n        nav_system.get_logger().info(\'Navigation system interrupted\')\n    finally:\n        nav_system.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(e.h3,{id:"navigation-in-different-scenarios",children:"Navigation in Different Scenarios:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Indoor Navigation"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Use occupancy grid maps for path planning"}),"\n",(0,r.jsx)(e.li,{children:"Implement dynamic obstacle avoidance"}),"\n",(0,r.jsx)(e.li,{children:"Integrate with SLAM systems for unknown environments"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Outdoor Navigation"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Handle uneven terrain and slopes"}),"\n",(0,r.jsx)(e.li,{children:"Use GPS and IMU for localization"}),"\n",(0,r.jsx)(e.li,{children:"Implement weather-based navigation adjustments"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Humanoid Robot Navigation"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Plan paths considering robot's physical constraints"}),"\n",(0,r.jsx)(e.li,{children:"Implement safe human-robot interaction protocols"}),"\n",(0,r.jsx)(e.li,{children:"Use legged locomotion controllers for complex terrain"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"control-strategies",children:"Control Strategies:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# Example: PID Controller for Robot Control\nclass PIDController:\n    def __init__(self, kp, ki, kd, dt=0.01):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.dt = dt\n\n        self.prev_error = 0\n        self.integral = 0\n\n    def compute(self, setpoint, measurement):\n        error = setpoint - measurement\n\n        # Proportional term\n        p_term = self.kp * error\n\n        # Integral term\n        self.integral += error * self.dt\n        i_term = self.ki * self.integral\n\n        # Derivative term\n        derivative = (error - self.prev_error) / self.dt\n        d_term = self.kd * derivative\n\n        self.prev_error = error\n\n        return p_term + i_term + d_term\n"})}),"\n",(0,r.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety First"}),": Always implement safety checks and emergency stops"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Modular Design"}),": Separate perception, planning, and control components"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Parameter Tuning"}),": Carefully tune control parameters for your specific robot"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Testing"}),": Extensively test in simulation before real-world deployment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Documentation"}),": Maintain clear documentation of control algorithms"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance Monitoring"}),": Track control performance metrics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Error Handling"}),": Implement robust error handling and recovery"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Validation"}),": Validate control systems with various scenarios"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"required-tools--software",children:"Required Tools & Software"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Isaac Sim"}),": Latest version with navigation extensions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Navigation Stack"}),": For advanced navigation capabilities"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Python Libraries"}),": NumPy, OpenCV, SciPy for control algorithms"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"System Requirements"}),": Sufficient computational power for real-time control"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Development Environment"}),": Python IDE for control system development"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,r.jsx)(e.p,{children:"After completing this lesson, you should:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand how to implement robot control systems in Isaac Sim"}),"\n",(0,r.jsx)(e.li,{children:"Be able to create navigation behaviors with obstacle avoidance"}),"\n",(0,r.jsx)(e.li,{children:"Know how to integrate perception with control for closed-loop operation"}),"\n",(0,r.jsx)(e.li,{children:"Have experience with ROS 2 navigation stack integration"}),"\n",(0,r.jsx)(e.li,{children:"Be prepared to validate control systems in simulation before real-world deployment"}),"\n",(0,r.jsx)(e.li,{children:"Understand the principles of robot control for humanoid robotics applications"}),"\n"]})]})}function _(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}}}]);