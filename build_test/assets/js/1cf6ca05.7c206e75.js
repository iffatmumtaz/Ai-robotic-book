"use strict";(globalThis.webpackChunkq4_hackathon=globalThis.webpackChunkq4_hackathon||[]).push([[4670],{8033:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module3-nvidia-isaac/lesson2-isaac-perception-pipelines","title":"Lesson 2 - Perception Pipelines and Sensor Integration","description":"Overview","source":"@site/docs/module3-nvidia-isaac/lesson2-isaac-perception-pipelines.md","sourceDirName":"module3-nvidia-isaac","slug":"/module3-nvidia-isaac/lesson2-isaac-perception-pipelines","permalink":"/Q4-hackathon1/docs/module3-nvidia-isaac/lesson2-isaac-perception-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/iffatmumtaz/Q4-hackathon1/edit/main/docs/module3-nvidia-isaac/lesson2-isaac-perception-pipelines.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Isaac Sim Perception Pipelines","title":"Lesson 2 - Perception Pipelines and Sensor Integration"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/Q4-hackathon1/docs/module3-nvidia-isaac/lesson1-isaac-sim-intro"},"next":{"title":"Isaac Sim Robot Control","permalink":"/Q4-hackathon1/docs/module3-nvidia-isaac/lesson3-isaac-robot-control"}}');var a=t(4848),r=t(8453);const s={sidebar_label:"Isaac Sim Perception Pipelines",title:"Lesson 2 - Perception Pipelines and Sensor Integration"},o="Lesson 2: Perception Pipelines and Sensor Integration in Isaac Sim",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Goals",id:"learning-goals",level:2},{value:"Concepts",id:"concepts",level:2},{value:"Perception in Robotics",id:"perception-in-robotics",level:3},{value:"Isaac Sim Perception Capabilities:",id:"isaac-sim-perception-capabilities",level:3},{value:"Synthetic vs Real Data:",id:"synthetic-vs-real-data",level:3},{value:"Steps",id:"steps",level:2},{value:"Step 1: Understanding Isaac Sim Sensors",id:"step-1-understanding-isaac-sim-sensors",level:3},{value:"Step 2: Configuring Synthetic Data Generation",id:"step-2-configuring-synthetic-data-generation",level:3},{value:"Step 3: Creating a Perception Processing Pipeline",id:"step-3-creating-a-perception-processing-pipeline",level:3},{value:"Step 4: ROS 2 Integration for Perception Data",id:"step-4-ros-2-integration-for-perception-data",level:3},{value:"Code",id:"code",level:2},{value:"Complete Perception Pipeline with Isaac Sim and ROS Integration:",id:"complete-perception-pipeline-with-isaac-sim-and-ros-integration",level:3},{value:"Examples",id:"examples",level:2},{value:"Perception Pipeline in Different Scenarios:",id:"perception-pipeline-in-different-scenarios",level:3},{value:"Using Isaac Sim Extensions for Perception:",id:"using-isaac-sim-extensions-for-perception",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Required Tools &amp; Software",id:"required-tools--software",level:2},{value:"Expected Outcome",id:"expected-outcome",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lesson-2-perception-pipelines-and-sensor-integration-in-isaac-sim",children:"Lesson 2: Perception Pipelines and Sensor Integration in Isaac Sim"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This lesson focuses on creating perception pipelines in NVIDIA Isaac Sim, which are essential for developing computer vision and sensor processing algorithms. You'll learn how to configure sensors, generate synthetic data, and create perception pipelines that bridge simulation and real-world robotics applications."}),"\n",(0,a.jsx)(n.h2,{id:"learning-goals",children:"Learning Goals"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Configure various sensors in Isaac Sim (cameras, LIDAR, IMU, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Generate synthetic training data with ground truth annotations"}),"\n",(0,a.jsx)(n.li,{children:"Create perception pipelines for computer vision applications"}),"\n",(0,a.jsx)(n.li,{children:"Integrate perception outputs with ROS 2 for robotics workflows"}),"\n",(0,a.jsx)(n.li,{children:"Validate perception algorithms in simulation before real-world deployment"}),"\n",(0,a.jsx)(n.li,{children:"Understand the relationship between synthetic and real sensor data"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"concepts",children:"Concepts"}),"\n",(0,a.jsx)(n.h3,{id:"perception-in-robotics",children:"Perception in Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Perception is the ability of a robot to understand its environment through sensor data. In robotics, perception systems typically include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computer Vision"}),": Processing camera images for object detection, recognition, and scene understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range Sensing"}),": Using LIDAR, depth sensors, or stereo cameras for 3D mapping"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Inertial Sensing"}),": Using IMUs for orientation and motion tracking"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-sensor Fusion"}),": Combining data from multiple sensors for robust perception"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"isaac-sim-perception-capabilities",children:"Isaac Sim Perception Capabilities:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Create labeled training data with perfect ground truth"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Simulation"}),": Accurate simulation of cameras, LIDAR, IMU, and other sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),": Vary environments and objects to improve model generalization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ground Truth Annotation"}),": Automatic generation of semantic segmentation, depth, etc."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Realistic Rendering"}),": RTX-powered rendering for photorealistic data"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"synthetic-vs-real-data",children:"Synthetic vs Real Data:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic Advantages"}),": Perfect ground truth, unlimited data, controlled scenarios"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real Data Advantages"}),": Authentic sensor noise, real-world complexity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Gap"}),": Differences between synthetic and real data that must be bridged"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": Techniques to make synthetic-trained models work on real data"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"steps",children:"Steps"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-understanding-isaac-sim-sensors",children:"Step 1: Understanding Isaac Sim Sensors"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim provides various sensor types that can be attached to robots or placed in the environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Adding sensors to a robot in Isaac Sim\nfrom omni.isaac.sensor import Camera, LidarRtx\nimport numpy as np\n\n\ndef add_sensors_to_robot(robot_prim_path):\n    """Add various sensors to a robot in Isaac Sim"""\n\n    # Add RGB camera\n    camera = Camera(\n        prim_path=f"{robot_prim_path}/camera",\n        frequency=30,  # Hz\n        resolution=(640, 480)\n    )\n\n    # Add LIDAR sensor\n    lidar = LidarRtx(\n        prim_path=f"{robot_prim_path}/lidar",\n        translation=np.array([0.0, 0.0, 0.3]),\n        orientation=np.array([0, 0, 0, 1]),\n        config="Example_Rotary",\n        rotation_frequency=20,\n        samples_per_scan=1080\n    )\n\n    # Add IMU sensor\n    # IMU is typically part of the robot\'s base or specific link\n    return camera, lidar\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-configuring-synthetic-data-generation",children:"Step 2: Configuring Synthetic Data Generation"}),"\n",(0,a.jsx)(n.p,{children:"Set up Isaac Sim for synthetic data generation with ground truth:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# perception_pipeline_setup.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.synthetic_utils.sensors import Camera\nimport numpy as np\n\n\nclass PerceptionPipelineSetup:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.camera = None\n        self.synthetic_data_helper = None\n\n        self._setup_scene()\n        self._setup_camera()\n        self._setup_synthetic_data()\n\n    def _setup_scene(self):\n        """Setup the scene with objects for perception"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add a robot\n        robot_path = assets_root_path + "/Isaac/Robots/Franka/franka_instanceable.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Robot")\n\n        # Add objects for perception\n        from omni.isaac.core.objects import DynamicCuboid, DynamicSphere\n        self.world.scene.add(\n            DynamicCuboid(\n                prim_path="/World/Object1",\n                name="object1",\n                position=np.array([0.5, 0.5, 0.5]),\n                size=0.1,\n                color=np.array([0.5, 0, 0])\n            )\n        )\n\n        self.world.scene.add(\n            DynamicSphere(\n                prim_path="/World/Object2",\n                name="object2",\n                position=np.array([-0.3, 0.2, 0.4]),\n                radius=0.08,\n                color=np.array([0, 0.5, 0])\n            )\n        )\n\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n    def _setup_camera(self):\n        """Setup RGB camera for perception"""\n        # Create camera prim\n        self.camera = Camera(\n            prim_path="/World/Robot/camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Set camera position relative to robot\n        self.camera.set_translation(np.array([0.1, 0.0, 0.1]))\n        self.camera.set_orientation(np.array([0.5, 0.5, 0.5, 0.5]))  # Looking forward\n\n    def _setup_synthetic_data(self):\n        """Setup synthetic data generation"""\n        self.synthetic_data_helper = SyntheticDataHelper()\n\n        # Enable various ground truth data types\n        self.synthetic_data_helper.enable_data_type("rgb", device="cpu")\n        self.synthetic_data_helper.enable_data_type("depth", device="cpu")\n        self.synthetic_data_helper.enable_data_type("instance_segmentation", device="cpu")\n        self.synthetic_data_helper.enable_data_type("bounding_box_2d_tight", device="cpu")\n\n        print("Synthetic data generation enabled")\n\n    def capture_data(self):\n        """Capture synthetic data from the scene"""\n        # Wait for world to be ready\n        self.world.reset()\n\n        # Step the world to update sensors\n        self.world.step(render=True)\n\n        # Get synthetic data\n        try:\n            rgb_data = self.synthetic_data_helper.get_data("rgb")\n            depth_data = self.synthetic_data_helper.get_data("depth")\n            seg_data = self.synthetic_data_helper.get_data("instance_segmentation")\n            bbox_data = self.synthetic_data_helper.get_data("bounding_box_2d_tight")\n\n            return {\n                \'rgb\': rgb_data,\n                \'depth\': depth_data,\n                \'segmentation\': seg_data,\n                \'bounding_boxes\': bbox_data\n            }\n        except Exception as e:\n            print(f"Error capturing synthetic data: {e}")\n            return None\n\n    def run_data_collection(self, num_frames=100):\n        """Run data collection for specified number of frames"""\n        print(f"Collecting {num_frames} frames of synthetic data...")\n\n        collected_data = []\n        for i in range(num_frames):\n            data = self.capture_data()\n            if data:\n                collected_data.append(data)\n\n            if i % 20 == 0:\n                print(f"Collected {i}/{num_frames} frames")\n\n            # Move objects around to create variation\n            if i % 10 == 0:\n                self._move_objects()\n\n        print(f"Data collection completed. Collected {len(collected_data)} frames")\n        return collected_data\n\n    def _move_objects(self):\n        """Move objects to create variation in the scene"""\n        # In a real implementation, you would move objects to create diverse training data\n        pass\n\n\ndef main():\n    pipeline = PerceptionPipelineSetup()\n    data = pipeline.run_data_collection(num_frames=50)\n    print(f"Collected {len(data)} frames of synthetic perception data")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-creating-a-perception-processing-pipeline",children:"Step 3: Creating a Perception Processing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# perception_processing.py\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport omni\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\n\n\nclass PerceptionProcessor:\n    """Process synthetic perception data and simulate real perception algorithms"""\n\n    def __init__(self):\n        self.object_detector = None\n        self.segmentation_model = None\n        self.depth_processor = None\n\n    def process_rgb_image(self, rgb_data):\n        """Process RGB image data"""\n        # Convert synthetic RGB data to OpenCV format\n        image = self._convert_synthetic_to_cv2(rgb_data)\n\n        # Apply computer vision algorithms\n        processed_image = self._apply_cv_algorithms(image)\n\n        return processed_image\n\n    def process_depth_data(self, depth_data):\n        """Process depth data for 3D understanding"""\n        # Process depth information\n        depth_map = self._extract_depth_features(depth_data)\n\n        # Generate point cloud or 3D information\n        point_cloud = self._depth_to_pointcloud(depth_map)\n\n        return point_cloud\n\n    def process_segmentation(self, seg_data):\n        """Process instance segmentation data"""\n        # Extract object instances from segmentation\n        instances = self._extract_instances(seg_data)\n\n        # Generate bounding boxes and object information\n        objects = self._instances_to_objects(instances)\n\n        return objects\n\n    def _convert_synthetic_to_cv2(self, rgb_data):\n        """Convert Isaac Sim RGB data to OpenCV format"""\n        # Isaac Sim typically provides data as numpy arrays\n        # Convert from (H, W, C) with values 0-1 to OpenCV format\n        image = (rgb_data * 255).astype(np.uint8)\n        return image\n\n    def _apply_cv_algorithms(self, image):\n        """Apply computer vision algorithms to the image"""\n        # Example: Simple edge detection\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Example: Color-based object detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Define color ranges for object detection\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask_red1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask_red2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask_red = mask_red1 + mask_red2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw contours on image\n        result = image.copy()\n        cv2.drawContours(result, contours, -1, (0, 255, 0), 2)\n\n        return result\n\n    def _extract_depth_features(self, depth_data):\n        """Extract features from depth data"""\n        # Depth data processing\n        # Example: Find surfaces, obstacles, or planar regions\n        valid_depth = depth_data > 0\n        depth_features = {\n            \'min_depth\': np.min(depth_data[valid_depth]) if np.any(valid_depth) else float(\'inf\'),\n            \'max_depth\': np.max(depth_data[valid_depth]) if np.any(valid_depth) else 0,\n            \'mean_depth\': np.mean(depth_data[valid_depth]) if np.any(valid_depth) else 0,\n            \'depth_variance\': np.var(depth_data[valid_depth]) if np.any(valid_depth) else 0\n        }\n        return depth_features\n\n    def _depth_to_pointcloud(self, depth_features):\n        """Convert depth information to 3D point cloud"""\n        # In a real implementation, this would create a full point cloud\n        # For this example, we\'ll return simplified 3D information\n        return {\n            \'has_obstacles\': depth_features[\'min_depth\'] < 1.0,  # Obstacle within 1 meter\n            \'surface_distance\': depth_features[\'mean_depth\'],\n            \'obstacle_density\': depth_features[\'depth_variance\']\n        }\n\n    def _extract_instances(self, seg_data):\n        """Extract individual object instances from segmentation"""\n        # Find unique instance IDs\n        unique_ids = np.unique(seg_data)\n        instances = {}\n\n        for instance_id in unique_ids:\n            if instance_id != 0:  # Skip background\n                mask = (seg_data == instance_id)\n                instances[instance_id] = mask\n\n        return instances\n\n    def _instances_to_objects(self, instances):\n        """Convert instances to object information"""\n        objects = []\n\n        for instance_id, mask in instances.items():\n            # Calculate bounding box\n            y_coords, x_coords = np.where(mask)\n            if len(x_coords) > 0 and len(y_coords) > 0:\n                bbox = {\n                    \'id\': instance_id,\n                    \'bbox\': [int(np.min(x_coords)), int(np.min(y_coords)),\n                            int(np.max(x_coords)), int(np.max(y_coords))],\n                    \'center\': [int(np.mean(x_coords)), int(np.mean(y_coords))],\n                    \'area\': len(x_coords)\n                }\n                objects.append(bbox)\n\n        return objects\n\n\ndef main():\n    # Example of using the perception processor\n    processor = PerceptionProcessor()\n\n    # Simulate processing synthetic data\n    # In a real scenario, this would come from Isaac Sim\n    synthetic_rgb = np.random.rand(480, 640, 3).astype(np.float32)  # Simulated RGB\n    synthetic_depth = np.random.rand(480, 640).astype(np.float32) * 10.0  # Simulated depth\n    synthetic_seg = np.random.randint(0, 5, (480, 640))  # Simulated segmentation\n\n    # Process the data\n    processed_image = processor.process_rgb_image(synthetic_rgb)\n    depth_info = processor.process_depth_data(synthetic_depth)\n    objects = processor.process_segmentation(synthetic_seg)\n\n    print(f"Processed image shape: {processed_image.shape}")\n    print(f"Depth info: {depth_info}")\n    print(f"Detected {len(objects)} objects")\n\n    for obj in objects:\n        print(f"  Object ID: {obj[\'id\']}, Area: {obj[\'area\']}")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-ros-2-integration-for-perception-data",children:"Step 4: ROS 2 Integration for Perception Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# isaac_perception_ros.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport struct\n\n\nclass IsaacPerceptionROSBridge(Node):\n    \"\"\"Bridge between Isaac Sim perception and ROS 2\"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_perception_bridge')\n\n        # Initialize CvBridge for image conversion\n        self.bridge = CvBridge()\n\n        # Publishers for perception data\n        self.rgb_pub = self.create_publisher(Image, '/isaac_sim/camera/rgb', 10)\n        self.depth_pub = self.create_publisher(Image, '/isaac_sim/camera/depth', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, '/isaac_sim/pointcloud', 10)\n\n        # Timer for publishing perception data\n        self.timer = self.create_timer(0.1, self.publish_perception_data)  # 10 Hz\n\n        self.frame_count = 0\n        self.get_logger().info('Isaac Perception ROS Bridge initialized')\n\n    def publish_perception_data(self):\n        \"\"\"Publish simulated perception data to ROS topics\"\"\"\n        # In a real implementation, this would get data from Isaac Sim\n        # For this example, we'll generate simulated data\n\n        # Publish RGB image\n        rgb_image = self._generate_simulated_rgb()\n        rgb_msg = self.bridge.cv2_to_imgmsg(rgb_image, encoding='rgb8')\n        rgb_msg.header.stamp = self.get_clock().now().to_msg()\n        rgb_msg.header.frame_id = 'camera_rgb_optical_frame'\n        self.rgb_pub.publish(rgb_msg)\n\n        # Publish depth image\n        depth_image = self._generate_simulated_depth()\n        depth_msg = self.bridge.cv2_to_imgmsg(depth_image, encoding='32FC1')\n        depth_msg.header.stamp = rgb_msg.header.stamp\n        depth_msg.header.frame_id = 'camera_depth_optical_frame'\n        self.depth_pub.publish(depth_msg)\n\n        # Publish point cloud\n        pointcloud_msg = self._generate_pointcloud_msg(depth_image, rgb_image)\n        pointcloud_msg.header.stamp = rgb_msg.header.stamp\n        pointcloud_msg.header.frame_id = 'camera_link'\n        self.pointcloud_pub.publish(pointcloud_msg)\n\n        self.frame_count += 1\n        if self.frame_count % 100 == 0:\n            self.get_logger().info(f'Published {self.frame_count} perception frames')\n\n    def _generate_simulated_rgb(self):\n        \"\"\"Generate simulated RGB image\"\"\"\n        # Create a simulated image with some geometric shapes\n        height, width = 480, 640\n        image = np.zeros((height, width, 3), dtype=np.uint8)\n\n        # Add some colored shapes to make it more realistic\n        cv2.rectangle(image, (100, 100), (200, 200), (255, 0, 0), -1)  # Blue square\n        cv2.circle(image, (300, 150), 50, (0, 255, 0), -1)  # Green circle\n        cv2.line(image, (0, 300), (640, 300), (0, 0, 255), 3)  # Red line\n\n        return image\n\n    def _generate_simulated_depth(self):\n        \"\"\"Generate simulated depth image\"\"\"\n        height, width = 480, 640\n        depth = np.ones((height, width), dtype=np.float32) * 5.0  # Default 5m depth\n\n        # Add some depth variation\n        for i in range(100, 200):\n            for j in range(100, 200):\n                depth[i, j] = 2.0  # Object at 2m\n\n        for i in range(300, 400):\n            for j in range(250, 350):\n                depth[i, j] = 1.5  # Object at 1.5m\n\n        return depth\n\n    def _generate_pointcloud_msg(self, depth_image, rgb_image):\n        \"\"\"Generate PointCloud2 message from depth and RGB data\"\"\"\n        height, width = depth_image.shape\n\n        # Create point cloud data\n        points = []\n        for v in range(height):\n            for u in range(width):\n                z = depth_image[v, u]\n                if z > 0 and z < 10:  # Valid depth range\n                    # Convert pixel to 3D point (simplified)\n                    x = (u - width/2) * z * 0.001  # Rough conversion\n                    y = (v - height/2) * z * 0.001\n                    r, g, b = rgb_image[v, u]\n\n                    points.append([x, y, z, r, g, b])\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\n            PointField(name='r', offset=12, datatype=PointField.UINT8, count=1),\n            PointField(name='g', offset=13, datatype=PointField.UINT8, count=1),\n            PointField(name='b', offset=14, datatype=PointField.UINT8, count=1),\n        ]\n\n        # Pack point data\n        point_data = []\n        for point in points:\n            # Pack as bytes\n            packed = struct.pack('fffBBB', point[0], point[1], point[2],\n                                int(point[3]), int(point[4]), int(point[5]))\n            point_data.append(packed)\n\n        # Create the message\n        pc2_msg = PointCloud2()\n        pc2_msg.height = 1\n        pc2_msg.width = len(point_data)\n        pc2_msg.fields = fields\n        pc2_msg.is_bigendian = False\n        pc2_msg.point_step = 17  # Size of each point (x,y,z,r,g,b + padding)\n        pc2_msg.row_step = pc2_msg.point_step * pc2_msg.width\n        pc2_msg.is_dense = True\n        pc2_msg.data = b''.join(point_data)\n\n        return pc2_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_bridge = IsaacPerceptionROSBridge()\n\n    try:\n        rclpy.spin(perception_bridge)\n    except KeyboardInterrupt:\n        perception_bridge.get_logger().info('Perception bridge interrupted')\n    finally:\n        perception_bridge.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"code",children:"Code"}),"\n",(0,a.jsx)(n.h3,{id:"complete-perception-pipeline-with-isaac-sim-and-ros-integration",children:"Complete Perception Pipeline with Isaac Sim and ROS Integration:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# complete_perception_pipeline.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.sensor import Camera\nimport numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport threading\nimport time\n\n\nclass IsaacSimPerceptionSystem(Node):\n    """Complete perception system integrating Isaac Sim with ROS 2"""\n\n    def __init__(self):\n        super().__init__(\'isaac_perception_system\')\n\n        # Initialize ROS components\n        self.bridge = CvBridge()\n\n        # Publishers for perception data\n        self.rgb_pub = self.create_publisher(Image, \'/perception/rgb\', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, \'/perception/camera_info\', 10)\n\n        # Initialize Isaac Sim components\n        self.world = World(stage_units_in_meters=1.0)\n        self.camera = None\n        self.synthetic_data_helper = None\n        self.isaac_initialized = False\n\n        # Setup Isaac Sim\n        self._setup_isaac_sim()\n\n        # Timer for perception loop\n        self.perception_timer = self.create_timer(0.033, self.perception_callback)  # ~30 Hz\n        self.frame_count = 0\n\n        self.get_logger().info(\'Isaac Sim Perception System initialized\')\n\n    def _setup_isaac_sim(self):\n        """Setup Isaac Sim environment and sensors"""\n        try:\n            # Setup scene\n            assets_root_path = get_assets_root_path()\n            if assets_root_path is None:\n                self.get_logger().error("Could not find Isaac Sim assets")\n                return\n\n            # Add a simple robot with camera\n            robot_path = assets_root_path + "/Isaac/Robots/Carter/carter.modelUSD.usd"\n            add_reference_to_stage(usd_path=robot_path, prim_path="/World/Robot")\n\n            # Add objects for perception\n            from omni.isaac.core.objects import DynamicCuboid\n            for i in range(5):\n                self.world.scene.add(\n                    DynamicCuboid(\n                        prim_path=f"/World/Object{i}",\n                        name=f"object{i}",\n                        position=np.array([i*0.5 - 1.0, 0.5, 0.5]),\n                        size=0.15,\n                        color=np.array([np.random.rand(), np.random.rand(), np.random.rand()])\n                    )\n                )\n\n            # Add ground plane\n            self.world.scene.add_default_ground_plane()\n\n            # Initialize world\n            self.world.reset()\n\n            # Add camera sensor\n            self.camera = Camera(\n                prim_path="/World/Robot/Looks/visual/camera",\n                frequency=30,\n                resolution=(640, 480)\n            )\n\n            # Alternative: Add camera to robot base if the above path doesn\'t work\n            if not self.camera.is_valid():\n                self.camera = Camera(\n                    prim_path="/World/Robot/camera",\n                    frequency=30,\n                    resolution=(640, 480)\n                )\n                self.camera.set_translation(np.array([0.2, 0.0, 0.1]))  # Position camera\n\n            # Setup synthetic data helper\n            self.synthetic_data_helper = SyntheticDataHelper()\n            self.synthetic_data_helper.enable_data_type("rgb", device="cpu")\n\n            self.isaac_initialized = True\n            self.get_logger().info("Isaac Sim perception system initialized successfully")\n\n        except Exception as e:\n            self.get_logger().error(f"Error setting up Isaac Sim: {e}")\n\n    def perception_callback(self):\n        """Main perception callback that runs at 30Hz"""\n        if not self.isaac_initialized:\n            return\n\n        try:\n            # Step Isaac Sim world\n            self.world.step(render=True)\n\n            # Get RGB data from Isaac Sim\n            if self.synthetic_data_helper and self.camera:\n                # Wait for data to be ready\n                time.sleep(0.01)  # Small delay to ensure data is captured\n\n                # Get synthetic RGB data\n                rgb_data = self.camera.get_rgb()\n\n                if rgb_data is not None and rgb_data.size > 0:\n                    # Convert to ROS Image message\n                    ros_image = self.bridge.cv2_to_imgmsg(rgb_data, encoding=\'rgb8\')\n                    ros_image.header.stamp = self.get_clock().now().to_msg()\n                    ros_image.header.frame_id = \'camera_rgb_optical_frame\'\n\n                    # Publish RGB image\n                    self.rgb_pub.publish(ros_image)\n\n                    # Publish camera info\n                    camera_info = self._create_camera_info()\n                    camera_info.header = ros_image.header\n                    self.camera_info_pub.publish(camera_info)\n\n                    self.frame_count += 1\n                    if self.frame_count % 100 == 0:\n                        self.get_logger().info(f\'Published {self.frame_count} perception frames\')\n\n        except Exception as e:\n            self.get_logger().error(f"Error in perception callback: {e}")\n\n    def _create_camera_info(self):\n        """Create camera info message"""\n        camera_info = CameraInfo()\n        camera_info.height = 480\n        camera_info.width = 640\n        camera_info.distortion_model = \'plumb_bob\'\n\n        # Example camera matrix (should match your camera configuration)\n        camera_info.k = [320.0, 0.0, 320.0,  # fx, 0, cx\n                         0.0, 320.0, 240.0,  # 0, fy, cy\n                         0.0, 0.0, 1.0]      # 0, 0, 1\n\n        # Example rectification matrix (identity for monocular camera)\n        camera_info.r = [1.0, 0.0, 0.0,\n                         0.0, 1.0, 0.0,\n                         0.0, 0.0, 1.0]\n\n        # Example projection matrix\n        camera_info.p = [320.0, 0.0, 320.0, 0.0,   # fx, 0, cx, 0\n                         0.0, 320.0, 240.0, 0.0,   # 0, fy, cy, 0\n                         0.0, 0.0, 1.0, 0.0]       # 0, 0, 1, 0\n\n        return camera_info\n\n\ndef main(args=None):\n    # Initialize ROS\n    rclpy.init(args=args)\n\n    # Create perception system\n    perception_system = IsaacSimPerceptionSystem()\n\n    try:\n        # Run ROS spin\n        rclpy.spin(perception_system)\n    except KeyboardInterrupt:\n        perception_system.get_logger().info(\'Perception system interrupted\')\n    finally:\n        perception_system.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(n.h3,{id:"perception-pipeline-in-different-scenarios",children:"Perception Pipeline in Different Scenarios:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Object Detection Training"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Generate thousands of synthetic images with labeled objects"}),"\n",(0,a.jsx)(n.li,{children:"Use domain randomization to improve model robustness"}),"\n",(0,a.jsx)(n.li,{children:"Transfer trained models to real robots"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"SLAM Development"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create synthetic environments with known layouts"}),"\n",(0,a.jsx)(n.li,{children:"Test SLAM algorithms with perfect ground truth"}),"\n",(0,a.jsx)(n.li,{children:"Validate mapping and localization performance"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Navigation Testing"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Simulate various obstacle configurations"}),"\n",(0,a.jsx)(n.li,{children:"Test perception in challenging lighting conditions"}),"\n",(0,a.jsx)(n.li,{children:"Validate safe navigation behaviors"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"using-isaac-sim-extensions-for-perception",children:"Using Isaac Sim Extensions for Perception:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example of using Isaac Sim extensions for advanced perception\nimport omni\nfrom omni.isaac.core.utils.extensions import enable_extension\n\n\ndef setup_perception_extensions():\n    """Setup Isaac Sim extensions for perception tasks"""\n\n    # Enable perception-related extensions\n    extensions_to_enable = [\n        \'omni.isaac.synthetic_utils\',\n        \'omni.isaac.range_sensor\',\n        \'omni.isaac.sensor\',\n        \'omni.isaac.ros2_bridge\'\n    ]\n\n    for ext in extensions_to_enable:\n        try:\n            enable_extension(ext)\n            print(f"Enabled extension: {ext}")\n        except Exception as e:\n            print(f"Failed to enable extension {ext}: {e}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),": Vary lighting, textures, and object positions to improve model generalization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Ground Truth Validation"}),": Always validate synthetic data quality against requirements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance Optimization"}),": Balance visual quality with simulation speed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Calibration"}),": Ensure simulated sensors match real hardware characteristics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Pipeline"}),": Create robust data collection and processing pipelines"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation"}),": Regularly test synthetic-trained models on real data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Documentation"}),": Maintain clear documentation of synthetic data generation parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Version Control"}),": Track perception pipeline configurations and parameters"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"required-tools--software",children:"Required Tools & Software"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim"}),": Latest version with perception extensions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CUDA"}),": Appropriate version for RTX rendering"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Python Libraries"}),": OpenCV, NumPy, PyTorch/TensorFlow for processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2"}),": For robotics integration workflows"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Requirements"}),": High-end GPU for realistic rendering"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Development Environment"}),": Python IDE for perception pipeline development"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,a.jsx)(n.p,{children:"After completing this lesson, you should:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand how to configure sensors in Isaac Sim for perception tasks"}),"\n",(0,a.jsx)(n.li,{children:"Be able to generate synthetic training data with ground truth annotations"}),"\n",(0,a.jsx)(n.li,{children:"Know how to create perception pipelines that integrate with ROS 2"}),"\n",(0,a.jsx)(n.li,{children:"Have experience with synthetic data generation for AI model training"}),"\n",(0,a.jsx)(n.li,{children:"Be prepared to validate perception algorithms in simulation before real-world deployment"}),"\n",(0,a.jsx)(n.li,{children:"Understand the relationship between synthetic and real sensor data"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);