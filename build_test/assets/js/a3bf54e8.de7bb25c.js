"use strict";(globalThis.webpackChunkq4_hackathon=globalThis.webpackChunkq4_hackathon||[]).push([[2567],{3925:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"assessment-materials","title":"Assessment and Quiz Materials","description":"Assessment and quiz materials for each module to validate learning outcomes in the Physical AI & Humanoid Robotics Book","source":"@site/docs/assessment-materials.md","sourceDirName":".","slug":"/assessment-materials","permalink":"/Q4-hackathon1/docs/assessment-materials","draft":false,"unlisted":false,"editUrl":"https://github.com/iffatmumtaz/Q4-hackathon1/edit/main/docs/assessment-materials.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Assessment and Quiz Materials","description":"Assessment and quiz materials for each module to validate learning outcomes in the Physical AI & Humanoid Robotics Book"}}');var t=s(4848),a=s(8453);const o={sidebar_position:9,title:"Assessment and Quiz Materials",description:"Assessment and quiz materials for each module to validate learning outcomes in the Physical AI & Humanoid Robotics Book"},r="Assessment and Quiz Materials",l={},c=[{value:"Assessment Philosophy",id:"assessment-philosophy",level:2},{value:"Learning Objectives Assessment",id:"learning-objectives-assessment",level:3},{value:"Assessment Levels",id:"assessment-levels",level:3},{value:"Module 1: ROS 2 Assessment Materials",id:"module-1-ros-2-assessment-materials",level:2},{value:"Pre-Assessment: ROS 2 Fundamentals",id:"pre-assessment-ros-2-fundamentals",level:3},{value:"Question 1: Basic Concepts (Multiple Choice)",id:"question-1-basic-concepts-multiple-choice",level:4},{value:"Question 2: Architecture Understanding (Short Answer)",id:"question-2-architecture-understanding-short-answer",level:4},{value:"Question 3: Practical Application (Scenario-Based)",id:"question-3-practical-application-scenario-based",level:4},{value:"Module Assessment: ROS 2 Basics",id:"module-assessment-ros-2-basics",level:3},{value:"Section A: Conceptual Understanding (30 points)",id:"section-a-conceptual-understanding-30-points",level:4},{value:"Section B: Practical Implementation (40 points)",id:"section-b-practical-implementation-40-points",level:4},{value:"Section C: Problem Solving (30 points)",id:"section-c-problem-solving-30-points",level:4},{value:"Post-Assessment: Module 1 Mastery",id:"post-assessment-module-1-mastery",level:3},{value:"Module 2: Gazebo/Unity Assessment Materials",id:"module-2-gazebounity-assessment-materials",level:2},{value:"Pre-Assessment: Simulation Fundamentals",id:"pre-assessment-simulation-fundamentals",level:3},{value:"Question 1: Basic Concepts (Multiple Choice)",id:"question-1-basic-concepts-multiple-choice-1",level:4},{value:"Question 2: Scenario Analysis (Short Answer)",id:"question-2-scenario-analysis-short-answer",level:4},{value:"Module Assessment: Gazebo/Unity Integration",id:"module-assessment-gazebounity-integration",level:3},{value:"Section A: Simulation Concepts (25 points)",id:"section-a-simulation-concepts-25-points",level:4},{value:"Section B: Practical Implementation (45 points)",id:"section-b-practical-implementation-45-points",level:4},{value:"Section C: Integration and Problem Solving (30 points)",id:"section-c-integration-and-problem-solving-30-points",level:4},{value:"Post-Assessment: Module 2 Mastery",id:"post-assessment-module-2-mastery",level:3},{value:"Module 3: NVIDIA Isaac Sim Assessment Materials",id:"module-3-nvidia-isaac-sim-assessment-materials",level:2},{value:"Pre-Assessment: Isaac Sim Concepts",id:"pre-assessment-isaac-sim-concepts",level:3},{value:"Question 1: Basic Concepts (Multiple Choice)",id:"question-1-basic-concepts-multiple-choice-2",level:4},{value:"Question 2: Scenario Analysis (Short Answer)",id:"question-2-scenario-analysis-short-answer-1",level:4},{value:"Module Assessment: Isaac Sim Integration",id:"module-assessment-isaac-sim-integration",level:3},{value:"Section A: Isaac Sim Concepts (25 points)",id:"section-a-isaac-sim-concepts-25-points",level:4},{value:"Section B: Practical Implementation (45 points)",id:"section-b-practical-implementation-45-points-1",level:4},{value:"Section C: Advanced Applications (30 points)",id:"section-c-advanced-applications-30-points",level:4},{value:"Post-Assessment: Module 3 Mastery",id:"post-assessment-module-3-mastery",level:3},{value:"Module 4: VLA Integration Assessment Materials",id:"module-4-vla-integration-assessment-materials",level:2},{value:"Pre-Assessment: VLA Concepts",id:"pre-assessment-vla-concepts",level:3},{value:"Question 1: Basic Concepts (Multiple Choice)",id:"question-1-basic-concepts-multiple-choice-3",level:4},{value:"Question 2: Scenario Analysis (Short Answer)",id:"question-2-scenario-analysis-short-answer-2",level:4},{value:"Module Assessment: VLA Integration",id:"module-assessment-vla-integration",level:3},{value:"Section A: VLA Concepts (25 points)",id:"section-a-vla-concepts-25-points",level:4},{value:"Section B: Practical Implementation (45 points)",id:"section-b-practical-implementation-45-points-2",level:4},{value:"Section C: Advanced VLA Systems (30 points)",id:"section-c-advanced-vla-systems-30-points",level:4},{value:"Post-Assessment: Module 4 Mastery",id:"post-assessment-module-4-mastery",level:3},{value:"Capstone Project Assessment",id:"capstone-project-assessment",level:2},{value:"Integrated Assessment: Autonomous Humanoid System",id:"integrated-assessment-autonomous-humanoid-system",level:3},{value:"Assessment Criteria (100 points total):",id:"assessment-criteria-100-points-total",level:4},{value:"Final Competency Assessment",id:"final-competency-assessment",level:3},{value:"Assessment Rubric",id:"assessment-rubric",level:2},{value:"Scoring Scale:",id:"scoring-scale",level:3},{value:"Grading Components:",id:"grading-components",level:3},{value:"Self-Assessment Tools",id:"self-assessment-tools",level:2},{value:"Module Completion Checklists:",id:"module-completion-checklists",level:3},{value:"Confidence Ratings:",id:"confidence-ratings",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"assessment-and-quiz-materials",children:"Assessment and Quiz Materials"})}),"\n",(0,t.jsx)(n.p,{children:"This document provides comprehensive assessment and quiz materials for each module in the Physical AI & Humanoid Robotics Book, designed to validate learning outcomes and ensure students have mastered the required concepts and skills."}),"\n",(0,t.jsx)(n.h2,{id:"assessment-philosophy",children:"Assessment Philosophy"}),"\n",(0,t.jsx)(n.h3,{id:"learning-objectives-assessment",children:"Learning Objectives Assessment"}),"\n",(0,t.jsx)(n.p,{children:"Each assessment is aligned with the specific learning objectives of each module, ensuring that students demonstrate:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understanding"}),": Grasp of core concepts and principles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Application"}),": Ability to apply knowledge to practical scenarios"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analysis"}),": Capacity to evaluate and troubleshoot complex systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthesis"}),": Ability to integrate concepts across modules"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"assessment-levels",children:"Assessment Levels"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Level 1 - Comprehension"}),": Basic understanding of concepts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Level 2 - Application"}),": Practical implementation of concepts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Level 3 - Analysis"}),": Problem-solving and troubleshooting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Level 4 - Synthesis"}),": Integration of multiple concepts"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-1-ros-2-assessment-materials",children:"Module 1: ROS 2 Assessment Materials"}),"\n",(0,t.jsx)(n.h3,{id:"pre-assessment-ros-2-fundamentals",children:"Pre-Assessment: ROS 2 Fundamentals"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Evaluate baseline knowledge before starting Module 1"]}),"\n",(0,t.jsx)(n.h4,{id:"question-1-basic-concepts-multiple-choice",children:"Question 1: Basic Concepts (Multiple Choice)"}),"\n",(0,t.jsx)(n.p,{children:"Which of the following is the primary communication mechanism in ROS 2 for continuous data streams?\nA) Services\nB) Actions\nC) Topics\nD) Parameters"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": C) Topics"]}),"\n",(0,t.jsx)(n.h4,{id:"question-2-architecture-understanding-short-answer",children:"Question 2: Architecture Understanding (Short Answer)"}),"\n",(0,t.jsx)(n.p,{children:"Explain the difference between a ROS 2 node and a ROS 2 package."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": A ROS 2 node is a running process that performs computation, while a package is a collection of nodes, libraries, and other resources that can be built and shared."]}),"\n",(0,t.jsx)(n.h4,{id:"question-3-practical-application-scenario-based",children:"Question 3: Practical Application (Scenario-Based)"}),"\n",(0,t.jsx)(n.p,{children:"You need to create a system where a sensor publishes temperature data every second, and a controller subscribes to this data to adjust heating. Describe the ROS 2 components you would need and their roles."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Publisher node: Reads temperature sensor and publishes to a temperature topic"}),"\n",(0,t.jsx)(n.li,{children:"Subscriber node: Subscribes to temperature topic and adjusts heating control"}),"\n",(0,t.jsx)(n.li,{children:"Topic: Named channel for temperature data transmission"}),"\n",(0,t.jsx)(n.li,{children:"Message type: Standardized format for temperature data"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"module-assessment-ros-2-basics",children:"Module Assessment: ROS 2 Basics"}),"\n",(0,t.jsx)(n.h4,{id:"section-a-conceptual-understanding-30-points",children:"Section A: Conceptual Understanding (30 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 1"})," (10 points): Compare and contrast the three main ROS 2 communication patterns (Topics, Services, Actions). Provide one use case for each."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Topics"}),": Asynchronous, continuous data streaming (e.g., sensor data, robot state)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Services"}),": Synchronous request-response (e.g., saving map, getting robot pose)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actions"}),": Asynchronous with feedback and goal management (e.g., navigation, manipulation)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 2"})," (10 points): Explain the ROS 2 lifecycle and why it's important for complex systems."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),": The ROS 2 lifecycle manages node states (unconfigured, inactive, active, finalized) allowing for coordinated startup, shutdown, and reconfiguration of complex systems with dependencies."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 3"})," (10 points): Describe the purpose and benefits of ROS 2 parameters."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),": Parameters allow runtime configuration of nodes, enable centralized configuration management, and provide a way to adjust behavior without recompiling code."]}),"\n",(0,t.jsx)(n.h4,{id:"section-b-practical-implementation-40-points",children:"Section B: Practical Implementation (40 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 4"})," (20 points): Write a Python ROS 2 publisher that publishes a custom message containing a timestamp and a string message every 2 seconds. Include the message definition and publisher code."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Custom message definition (msg/CustomMessage.msg)\n# ---\n# time timestamp\n# string message\n\n# Publisher code\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport time\n\nclass CustomPublisher(Node):\n    def __init__(self):\n        super().__init__('custom_publisher')\n        self.publisher = self.create_publisher(String, 'custom_topic', 10)\n        self.timer = self.create_timer(2.0, self.timer_callback)\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = f'Message at {time.time()}'\n        self.publisher.publish(msg)\n\ndef main():\n    rclpy.init()\n    node = CustomPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 5"})," (20 points): Design a ROS 2 launch file that starts a camera driver node and an image processing node, with the image processing node subscribing to the camera's image topic."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# launch/camera_processing.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='camera_driver',\n            executable='camera_node',\n            name='camera_driver',\n            parameters=[{'frame_id': 'camera'}]\n        ),\n        Node(\n            package='image_processing',\n            executable='processor_node',\n            name='image_processor',\n            remappings=[('image_in', 'camera/image_raw')]\n        )\n    ])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"section-c-problem-solving-30-points",children:"Section C: Problem Solving (30 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 6"})," (15 points): A colleague's ROS 2 system has intermittent communication failures. List five potential causes and how you would diagnose each."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network issues"}),": Check network connectivity and latency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain ID conflicts"}),": Verify ROS_DOMAIN_ID consistency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RMW implementation"}),": Ensure same middleware across systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Firewall blocking"}),": Check firewall rules for ROS ports"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource limitations"}),": Monitor CPU/memory usage"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 7"})," (15 points): Design a fault-tolerant ROS 2 system architecture for a robot that must continue operating even if one of its perception nodes fails."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement node monitoring with lifecycle nodes"}),"\n",(0,t.jsx)(n.li,{children:"Use multiple perception nodes with voting/selection logic"}),"\n",(0,t.jsx)(n.li,{children:"Include fallback algorithms when primary nodes fail"}),"\n",(0,t.jsx)(n.li,{children:"Implement heartbeat monitoring for critical components"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"post-assessment-module-1-mastery",children:"Post-Assessment: Module 1 Mastery"}),"\n",(0,t.jsx)(n.p,{children:"Students should be able to:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create and configure ROS 2 nodes"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement different communication patterns"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design launch files for complex systems"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Troubleshoot communication issues"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Apply lifecycle management"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-2-gazebounity-assessment-materials",children:"Module 2: Gazebo/Unity Assessment Materials"}),"\n",(0,t.jsx)(n.h3,{id:"pre-assessment-simulation-fundamentals",children:"Pre-Assessment: Simulation Fundamentals"}),"\n",(0,t.jsx)(n.h4,{id:"question-1-basic-concepts-multiple-choice-1",children:"Question 1: Basic Concepts (Multiple Choice)"}),"\n",(0,t.jsx)(n.p,{children:"What is the primary purpose of a digital twin in robotics?\nA) To replace the physical robot completely\nB) To create a virtual replica for simulation and testing\nC) To improve the robot's appearance\nD) To reduce hardware costs"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": B) To create a virtual replica for simulation and testing"]}),"\n",(0,t.jsx)(n.h4,{id:"question-2-scenario-analysis-short-answer",children:"Question 2: Scenario Analysis (Short Answer)"}),"\n",(0,t.jsx)(n.p,{children:"Explain why simulation is crucial for testing robot navigation systems before deploying on physical hardware."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": Simulation allows for safe testing of navigation algorithms, reduces risk of hardware damage, enables testing in various scenarios, and provides a controlled environment for debugging."]}),"\n",(0,t.jsx)(n.h3,{id:"module-assessment-gazebounity-integration",children:"Module Assessment: Gazebo/Unity Integration"}),"\n",(0,t.jsx)(n.h4,{id:"section-a-simulation-concepts-25-points",children:"Section A: Simulation Concepts (25 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 1"})," (10 points): Compare the advantages and disadvantages of Gazebo Classic vs. Gazebo Garden for robotics simulation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo Classic"}),": Mature, stable, extensive documentation; but outdated graphics, limited physics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo Garden"}),": Modern architecture, better graphics, improved physics; but newer, evolving API"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 2"})," (15 points): Design a simulation environment for testing a mobile robot's navigation capabilities. Include at least 5 different elements and explain their purpose."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Static obstacles"}),": Test path planning and obstacle avoidance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic obstacles"}),": Test real-time path replanning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Different floor textures"}),": Test traction and wheel slippage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lighting variations"}),": Test perception system robustness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Narrow passages"}),": Test navigation in constrained spaces"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"section-b-practical-implementation-45-points",children:"Section B: Practical Implementation (45 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 3"})," (25 points): Create a SDF model file for a simple differential drive robot with two wheels, a caster, and a RGB-D camera."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<sdf version="1.7">\n  <model name="simple_robot">\n    <link name="chassis">\n      <pose>0 0 0.1 0 0 0</pose>\n      <inertial>\n        <mass>1.0</mass>\n        <inertia>\n          <ixx>0.01</ixx>\n          <ixy>0</ixy>\n          <ixz>0</ixz>\n          <iyy>0.01</iyy>\n          <iyz>0</iyz>\n          <izz>0.01</izz>\n        </inertia>\n      </inertial>\n\n      <visual name="chassis_visual">\n        <geometry>\n          <box>\n            <size>0.5 0.3 0.2</size>\n          </box>\n        </geometry>\n      </visual>\n\n      <collision name="chassis_collision">\n        <geometry>\n          <box>\n            <size>0.5 0.3 0.2</size>\n          </box>\n        </geometry>\n      </collision>\n    </link>\n\n    \x3c!-- Left wheel --\x3e\n    <joint name="left_wheel_joint" type="revolute">\n      <parent>chassis</parent>\n      <child>left_wheel</child>\n      <axis>\n        <xyz>0 1 0</xyz>\n      </axis>\n    </joint>\n\n    <link name="left_wheel">\n      <pose>-0.15 0.2 0 0 0 0</pose>\n      <inertial>\n        <mass>0.2</mass>\n        <inertia>\n          <ixx>0.001</ixx>\n          <ixy>0</ixy>\n          <ixz>0</ixz>\n          <iyy>0.001</iyy>\n          <iyz>0</iyz>\n          <izz>0.001</izz>\n        </inertia>\n      </inertial>\n    </link>\n\n    \x3c!-- Right wheel (similar to left) --\x3e\n    <joint name="right_wheel_joint" type="revolute">\n      <parent>chassis</parent>\n      <child>right_wheel</child>\n      <axis>\n        <xyz>0 1 0</xyz>\n      </axis>\n    </joint>\n\n    <link name="right_wheel">\n      <pose>-0.15 -0.2 0 0 0 0</pose>\n      <inertial>\n        <mass>0.2</mass>\n        <inertia>\n          <ixx>0.001</ixx>\n          <ixy>0</ixy>\n          <ixz>0</ixz>\n          <iyy>0.001</iyy>\n          <iyz>0</iyz>\n          <izz>0.001</izz>\n        </inertia>\n      </inertial>\n    </link>\n\n    \x3c!-- RGB-D Camera --\x3e\n    <joint name="camera_joint" type="fixed">\n      <parent>chassis</parent>\n      <child>camera_link</child>\n    </joint>\n\n    <link name="camera_link">\n      <pose>0.2 0 0.1 0 0 0</pose>\n      <sensor name="rgbd_camera" type="rgbd_camera">\n        <camera>\n          <horizontal_fov>1.047</horizontal_fov>\n          <image>\n            <width>640</width>\n            <height>480</height>\n          </image>\n        </camera>\n      </sensor>\n    </link>\n  </model>\n</sdf>\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 4"})," (20 points): Write a Gazebo plugin that periodically publishes the robot's position to a ROS 2 topic."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:'#include <gazebo/gazebo.hh>\n#include <gazebo/physics/physics.hh>\n#include <gazebo/common/common.hh>\n#include <ros/ros.h>\n#include <geometry_msgs/Pose.h>\n\nnamespace gazebo\n{\n  class PositionPublisher : public ModelPlugin\n  {\n    public: void Load(physics::ModelPtr _model, sdf::ElementPtr _sdf)\n    {\n      this->model = _model;\n\n      // Initialize ROS\n      if (!ros::isInitialized())\n      {\n        int argc = 0;\n        char **argv = NULL;\n        ros::init(argc, argv, "gazebo_client",\n                  ros::init_options::NoSigintHandler);\n      }\n\n      this->rosNode.reset(new ros::NodeHandle());\n      this->pub = this->rosNode->advertise<geometry_msgs::Pose>("robot_pose", 1);\n\n      this->updateConnection = event::Events::ConnectWorldUpdateBegin(\n          boost::bind(&PositionPublisher::OnUpdate, this, _1));\n    }\n\n    public: void OnUpdate(const common::UpdateInfo & _info)\n    {\n      math::Pose pose = this->model->GetWorldPose();\n\n      geometry_msgs::Pose ros_pose;\n      ros_pose.position.x = pose.pos.x;\n      ros_pose.position.y = pose.pos.y;\n      ros_pose.position.z = pose.pos.z;\n\n      ros_pose.orientation.x = pose.rot.x;\n      ros_pose.orientation.y = pose.rot.y;\n      ros_pose.orientation.z = pose.rot.z;\n      ros_pose.orientation.w = pose.rot.w;\n\n      this->pub.publish(ros_pose);\n    }\n\n    private: physics::ModelPtr model;\n    private: ros::NodeHandlePtr rosNode;\n    private: ros::Publisher pub;\n    private: event::ConnectionPtr updateConnection;\n  };\n\n  GZ_REGISTER_MODEL_PLUGIN(PositionPublisher)\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"section-c-integration-and-problem-solving-30-points",children:"Section C: Integration and Problem Solving (30 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 5"})," (15 points): Explain the simulation-to-reality gap and strategies to minimize it."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),": The sim-to-real gap includes differences in physics simulation, sensor noise, and environmental factors. Strategies include domain randomization, system identification, and iterative testing."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 6"})," (15 points): Design a comprehensive testing suite for a robot navigation system using simulation. Include at least 6 different test scenarios."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open space navigation"}),": Test basic path following"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cluttered environment"}),": Test obstacle avoidance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Narrow passages"}),": Test precise navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic obstacles"}),": Test real-time replanning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor noise"}),": Test robustness to imperfect data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Failure scenarios"}),": Test recovery from navigation failures"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"post-assessment-module-2-mastery",children:"Post-Assessment: Module 2 Mastery"}),"\n",(0,t.jsx)(n.p,{children:"Students should be able to:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create detailed robot models for simulation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement physics-accurate simulations"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate simulation with ROS 2 systems"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design comprehensive testing scenarios"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Address simulation-to-reality challenges"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-3-nvidia-isaac-sim-assessment-materials",children:"Module 3: NVIDIA Isaac Sim Assessment Materials"}),"\n",(0,t.jsx)(n.h3,{id:"pre-assessment-isaac-sim-concepts",children:"Pre-Assessment: Isaac Sim Concepts"}),"\n",(0,t.jsx)(n.h4,{id:"question-1-basic-concepts-multiple-choice-2",children:"Question 1: Basic Concepts (Multiple Choice)"}),"\n",(0,t.jsx)(n.p,{children:"What is the primary advantage of NVIDIA Isaac Sim over traditional simulation platforms?\nA) Lower cost\nB) Tighter integration with NVIDIA hardware and AI tools\nC) Simpler interface\nD) Better documentation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": B) Tighter integration with NVIDIA hardware and AI tools"]}),"\n",(0,t.jsx)(n.h4,{id:"question-2-scenario-analysis-short-answer-1",children:"Question 2: Scenario Analysis (Short Answer)"}),"\n",(0,t.jsx)(n.p,{children:"Why is Isaac Sim particularly well-suited for AI-powered robotics applications?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": Isaac Sim provides high-fidelity rendering, photorealistic environments, built-in AI tools, and seamless integration with NVIDIA's AI ecosystem."]}),"\n",(0,t.jsx)(n.h3,{id:"module-assessment-isaac-sim-integration",children:"Module Assessment: Isaac Sim Integration"}),"\n",(0,t.jsx)(n.h4,{id:"section-a-isaac-sim-concepts-25-points",children:"Section A: Isaac Sim Concepts (25 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 1"})," (10 points): Explain the difference between synthetic data generation and domain randomization in Isaac Sim."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Creating realistic training data with perfect annotations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": Randomizing environment parameters to improve sim-to-real transfer"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 2"})," (15 points): Design an Isaac Sim environment for training a robot to perform object manipulation tasks. Include at least 4 key features and explain their purpose."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physics-accurate objects"}),": Realistic manipulation dynamics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Photorealistic rendering"}),": Domain randomization for vision systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Force/torque sensors"}),": Accurate contact feedback"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lighting variations"}),": Robust perception under different conditions"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"section-b-practical-implementation-45-points-1",children:"Section B: Practical Implementation (45 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 3"})," (25 points): Create a Python script using Isaac Sim's extension system to control a robotic arm to pick and place objects."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import omni\nimport carb\nimport numpy as np\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.franka import Franka\nfrom omni.isaac.core.objects import DynamicCuboid\n\n# Initialize Isaac Sim\nworld = World(stage_units_in_meters=1.0)\n\n# Add robot and objects\nfranka = world.scene.add(Franka(prim_path="/World/Franka", name="franka"))\ncube = world.scene.add(DynamicCuboid(prim_path="/World/cube", name="cube",\n                                  position=np.array([0.5, 0.0, 0.1]),\n                                  size=0.05, color=np.array([0.8, 0.1, 0.1])))\n\n# Add target location\ntarget = world.scene.add(DynamicCuboid(prim_path="/World/target", name="target",\n                                     position=np.array([0.5, 0.3, 0.1]),\n                                     size=0.05, color=np.array([0.1, 0.8, 0.1])))\n\nworld.reset()\n\n# Simple pick and place implementation\ndef pick_and_place():\n    # Move to cube position\n    franka.get_articulation_controller().apply_pos_cmd(np.array([0.5, 0.0, 0.3]))\n    world.step(render=True)\n\n    # Close gripper to pick cube\n    franka.apply_world_space_force_to_prim_at_position(force=np.array([0, 0, -10]), position=np.array([0.5, 0.0, 0.1]))\n    world.step(render=True)\n\n    # Lift cube\n    franka.get_articulation_controller().apply_pos_cmd(np.array([0.5, 0.0, 0.4]))\n    world.step(render=True)\n\n    # Move to target position\n    franka.get_articulation_controller().apply_pos_cmd(np.array([0.5, 0.3, 0.4]))\n    world.step(render=True)\n\n    # Release cube\n    franka.apply_world_space_force_to_prim_at_position(force=np.array([0, 0, 10]), position=np.array([0.5, 0.3, 0.1]))\n    world.step(render=True)\n\n# Execute task\nfor i in range(100):\n    pick_and_place()\n    world.step(render=True)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 4"})," (20 points): Design a perception pipeline in Isaac Sim that detects and localizes objects using computer vision."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.vision.sensors import Camera\nimport numpy as np\nimport cv2\n\nclass PerceptionPipeline:\n    def __init__(self, camera_path):\n        self.camera = Camera(prim_path=camera_path)\n        self.camera.initialize()\n\n    def detect_objects(self):\n        # Get RGB image\n        rgb_data = self.camera.get_rgb()\n\n        # Get depth image\n        depth_data = self.camera.get_depth()\n\n        # Process images using OpenCV\n        image = rgb_data.astype(np.uint8)\n\n        # Example: Detect red objects\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Calculate positions\n        object_positions = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small objects\n                M = cv2.moments(contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n\n                    # Convert pixel coordinates to world coordinates using depth\n                    depth_value = depth_data[cy, cx]\n                    world_x = cx * depth_value / 1000.0  # Simplified conversion\n                    world_y = cy * depth_value / 1000.0\n\n                    object_positions.append((world_x, world_y, depth_value))\n\n        return object_positions\n'})}),"\n",(0,t.jsx)(n.h4,{id:"section-c-advanced-applications-30-points",children:"Section C: Advanced Applications (30 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 5"})," (15 points): Explain how to implement reinforcement learning training in Isaac Sim for a robotic manipulation task."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define reward function based on task success"}),"\n",(0,t.jsx)(n.li,{children:"Implement observation space with relevant robot and object states"}),"\n",(0,t.jsx)(n.li,{children:"Use Isaac Gym for parallel training environments"}),"\n",(0,t.jsx)(n.li,{children:"Apply domain randomization to improve generalization"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 6"})," (15 points): Design a comprehensive testing protocol for verifying Isaac Sim's accuracy compared to real-world robot performance."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Compare kinematic accuracy between sim and real robot"}),"\n",(0,t.jsx)(n.li,{children:"Validate dynamic behaviors and forces"}),"\n",(0,t.jsx)(n.li,{children:"Test sensor data fidelity"}),"\n",(0,t.jsx)(n.li,{children:"Measure timing and latency differences"}),"\n",(0,t.jsx)(n.li,{children:"Validate control system performance"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"post-assessment-module-3-mastery",children:"Post-Assessment: Module 3 Mastery"}),"\n",(0,t.jsx)(n.p,{children:"Students should be able to:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create complex Isaac Sim environments"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement perception and control pipelines"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design reinforcement learning experiments"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validate simulation accuracy"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Address sim-to-real transfer challenges"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-4-vla-integration-assessment-materials",children:"Module 4: VLA Integration Assessment Materials"}),"\n",(0,t.jsx)(n.h3,{id:"pre-assessment-vla-concepts",children:"Pre-Assessment: VLA Concepts"}),"\n",(0,t.jsx)(n.h4,{id:"question-1-basic-concepts-multiple-choice-3",children:"Question 1: Basic Concepts (Multiple Choice)"}),"\n",(0,t.jsx)(n.p,{children:"What does VLA stand for in the context of robotics?\nA) Vision-Language-Action\nB) Vector-Linear-Algebra\nC) Variable-Length-Arrays\nD) Virtual-Learning-Agent"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": A) Vision-Language-Action"]}),"\n",(0,t.jsx)(n.h4,{id:"question-2-scenario-analysis-short-answer-2",children:"Question 2: Scenario Analysis (Short Answer)"}),"\n",(0,t.jsx)(n.p,{children:"Explain the main challenge in integrating Large Language Models with robotic systems."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer"}),": The main challenge is bridging the gap between high-level natural language commands and low-level robot actions, ensuring safety, and maintaining real-time performance."]}),"\n",(0,t.jsx)(n.h3,{id:"module-assessment-vla-integration",children:"Module Assessment: VLA Integration"}),"\n",(0,t.jsx)(n.h4,{id:"section-a-vla-concepts-25-points",children:"Section A: VLA Concepts (25 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 1"})," (10 points): Describe the three main components of Vision-Language-Action systems and their roles."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Perceiving and understanding the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Processing natural language commands and generating responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Executing physical actions in the real world"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 2"})," (15 points): Explain the safety considerations that must be addressed when integrating LLMs with physical robots."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Command validation and safety filtering"}),"\n",(0,t.jsx)(n.li,{children:"Real-time safety monitoring"}),"\n",(0,t.jsx)(n.li,{children:"Emergency stop mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Human-in-the-loop validation"}),"\n",(0,t.jsx)(n.li,{children:"Fail-safe behaviors"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"section-b-practical-implementation-45-points-2",children:"Section B: Practical Implementation (45 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 3"})," (25 points): Design a complete VLA system architecture that receives voice commands, processes them through an LLM, and executes robot actions with safety validation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport speech_recognition as sr\nimport openai\nimport threading\nimport time\n\nclass VLASystem(Node):\n    def __init__(self):\n        super().__init__('vla_system')\n\n        # Initialize components\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.safety_manager = SafetyManager()\n\n        # Publishers and subscribers\n        self.command_pub = self.create_publisher(String, 'robot_commands', 10)\n        self.safety_pub = self.create_publisher(String, 'safety_status', 10)\n\n        # Start voice recognition\n        self.voice_thread = threading.Thread(target=self.voice_recognition_loop, daemon=True)\n        self.voice_thread.start()\n\n    def voice_recognition_loop(self):\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0)\n\n                command_text = self.recognizer.recognize_google(audio)\n                self.process_command_safely(command_text)\n\n            except sr.WaitTimeoutError:\n                continue\n            except sr.UnknownValueError:\n                continue\n\n    def process_command_safely(self, command_text):\n        # Validate command with LLM\n        llm_response = self.query_llm(command_text)\n\n        # Parse action from response\n        action = self.parse_llm_response(llm_response)\n\n        # Validate safety constraints\n        if self.safety_manager.validate_action(action):\n            # Execute action\n            cmd_msg = String()\n            cmd_msg.data = action\n            self.command_pub.publish(cmd_msg)\n        else:\n            # Safety violation\n            self.get_logger().warn(f\"Safety violation in command: {command_text}\")\n\n    def query_llm(self, command):\n        # Query OpenAI or local LLM\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": command}]\n        )\n        return response.choices[0].message.content\n\n    def parse_llm_response(self, response):\n        # Parse structured action from LLM response\n        # Implementation depends on LLM output format\n        pass\n\nclass SafetyManager:\n    def __init__(self):\n        self.velocity_limit = 0.5  # m/s\n        self.proximity_threshold = 0.3  # meters\n\n    def validate_action(self, action):\n        # Check safety constraints\n        if action['type'] == 'move':\n            if action['velocity'] > self.velocity_limit:\n                return False\n        # Add more safety checks...\n        return True\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 4"})," (20 points): Create a multi-modal perception system that combines vision and language inputs for object detection and manipulation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport openai\nfrom PIL import Image\nimport io\n\nclass MultiModalPerception:\n    def __init__(self):\n        self.object_detector = ObjectDetector()\n        self.language_processor = LanguageProcessor()\n\n    def process_command_and_image(self, command, image):\n        # Process natural language command\n        target_object = self.language_processor.extract_object(command)\n\n        # Detect objects in image\n        detections = self.object_detector.detect(image)\n\n        # Match target object with detections\n        target_detections = self.match_target_object(detections, target_object)\n\n        if target_detections:\n            # Select best candidate\n            best_detection = self.select_best_candidate(target_detections)\n\n            # Generate action based on detection\n            action = self.generate_action(best_detection, command)\n\n            return action\n        else:\n            return {"error": "Target object not found"}\n\n    def match_target_object(self, detections, target_object):\n        matched_detections = []\n\n        for detection in detections:\n            # Use LLM to determine if detection matches target\n            prompt = f"Does this object description match \'{target_object}\'? Object: {detection[\'class\']} with attributes {detection[\'attributes\']}"\n\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}]\n            )\n\n            if "yes" in response.choices[0].message.content.lower():\n                matched_detections.append(detection)\n\n        return matched_detections\n\n    def select_best_candidate(self, detections):\n        # Select detection with highest confidence\n        return max(detections, key=lambda x: x[\'confidence\'])\n\n    def generate_action(self, detection, command):\n        # Generate robot action based on detection and command\n        center_x, center_y = detection[\'bbox_center\']\n        distance = detection[\'distance\']\n\n        return {\n            "action": "approach_and_grasp",\n            "position": {"x": center_x, "y": center_y},\n            "distance": distance,\n            "command": command\n        }\n\nclass ObjectDetector:\n    def detect(self, image):\n        # Use YOLO, Detectron2, or other detection model\n        # Return list of detections with bounding boxes, classes, and attributes\n        pass\n\nclass LanguageProcessor:\n    def extract_object(self, command):\n        # Extract target object from natural language command\n        # Use NLP techniques to identify object, attributes, and properties\n        pass\n'})}),"\n",(0,t.jsx)(n.h4,{id:"section-c-advanced-vla-systems-30-points",children:"Section C: Advanced VLA Systems (30 points)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 5"})," (15 points): Design a comprehensive safety protocol for VLA systems that operates at multiple levels (command, action, execution)."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultiLayerSafetyProtocol:\n    def __init__(self):\n        self.layer_1_basic = BasicSafetyConstraints()\n        self.layer_2_contextual = ContextualSafetyValidator()\n        self.layer_3_predictive = PredictiveSafetySystem()\n        self.layer_4_human = HumanOverrideSystem()\n\n    def validate_command(self, command, context):\n        # Layer 1: Basic safety checks\n        if not self.layer_1_basic.validate(command):\n            return False, "Basic safety violation"\n\n        # Layer 2: Contextual validation\n        if not self.layer_2_contextual.validate(command, context):\n            return False, "Contextual safety violation"\n\n        # Layer 3: Predictive safety\n        predicted_outcomes = self.layer_3_predictive.assess_risk(command, context)\n        if predicted_outcomes.risk_level > RiskLevel.HIGH:\n            return False, "High risk prediction"\n\n        # Layer 4: Human oversight\n        if self.layer_4_human.requires_approval(command):\n            return self.request_human_approval(command)\n\n        return True, "Command approved"\n\n    def request_human_approval(self, command):\n        # Implement human approval interface\n        pass\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Question 6"})," (15 points): Explain how to implement a fail-safe mechanism for VLA systems that handles LLM failures gracefully."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sample Answer"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement fallback responses when LLM is unavailable"}),"\n",(0,t.jsx)(n.li,{children:"Use cached or rule-based responses as backup"}),"\n",(0,t.jsx)(n.li,{children:"Maintain safe robot states during LLM failures"}),"\n",(0,t.jsx)(n.li,{children:"Log failures for analysis and improvement"}),"\n",(0,t.jsx)(n.li,{children:"Implement retry mechanisms with exponential backoff"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"post-assessment-module-4-mastery",children:"Post-Assessment: Module 4 Mastery"}),"\n",(0,t.jsx)(n.p,{children:"Students should be able to:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design complete VLA system architectures"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement multi-modal perception systems"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create comprehensive safety protocols"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handle LLM integration challenges"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validate VLA system performance and safety"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-assessment",children:"Capstone Project Assessment"}),"\n",(0,t.jsx)(n.h3,{id:"integrated-assessment-autonomous-humanoid-system",children:"Integrated Assessment: Autonomous Humanoid System"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),': Students must demonstrate mastery of all modules by implementing the capstone project: "Autonomous Humanoid with voice command \u2192 planning \u2192 navigation \u2192 object detection \u2192 manipulation"']}),"\n",(0,t.jsx)(n.h4,{id:"assessment-criteria-100-points-total",children:"Assessment Criteria (100 points total):"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Module Integration (40 points)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 communication between all components (10 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Simulation environment for testing (10 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac Sim integration for advanced perception (10 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","VLA system for natural language interaction (10 pts)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Technical Implementation (35 points)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Voice command processing (8 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Action planning and sequencing (9 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Navigation and path planning (9 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection and manipulation (9 pts)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Safety and Robustness (25 points)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Comprehensive safety protocols (10 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Error handling and recovery (8 pts)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance optimization (7 pts)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"final-competency-assessment",children:"Final Competency Assessment"}),"\n",(0,t.jsx)(n.p,{children:"Students should demonstrate competency in:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Designing integrated robotic systems"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implementing multi-modal perception"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Creating safe and robust systems"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Troubleshooting complex integration issues"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validating system performance"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"assessment-rubric",children:"Assessment Rubric"}),"\n",(0,t.jsx)(n.h3,{id:"scoring-scale",children:"Scoring Scale:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Excellent (A, 90-100%)"}),": Complete mastery, creative solutions, exceptional understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Good (B, 80-89%)"}),": Strong understanding, mostly correct implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Satisfactory (C, 70-79%)"}),": Basic understanding, partially correct"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Needs Improvement (D, 60-69%)"}),": Limited understanding, significant gaps"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unsatisfactory (F, Below 60%)"}),": Inadequate understanding"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"grading-components",children:"Grading Components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Theory Understanding"}),": 25%"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Practical Implementation"}),": 40%"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem Solving"}),": 20%"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation and Communication"}),": 15%"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"self-assessment-tools",children:"Self-Assessment Tools"}),"\n",(0,t.jsx)(n.h3,{id:"module-completion-checklists",children:"Module Completion Checklists:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Module 1: ROS 2 Basics - [ ] Complete"}),"\n",(0,t.jsx)(n.li,{children:"Module 2: Gazebo/Unity - [ ] Complete"}),"\n",(0,t.jsx)(n.li,{children:"Module 3: Isaac Sim - [ ] Complete"}),"\n",(0,t.jsx)(n.li,{children:"Module 4: VLA Integration - [ ] Complete"}),"\n",(0,t.jsx)(n.li,{children:"Capstone Project - [ ] Complete"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"confidence-ratings",children:"Confidence Ratings:"}),"\n",(0,t.jsx)(n.p,{children:"For each major concept, students should rate their confidence:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"5: Expert level, can teach others"}),"\n",(0,t.jsx)(n.li,{children:"4: Proficient, can implement independently"}),"\n",(0,t.jsx)(n.li,{children:"3: Competent, can implement with guidance"}),"\n",(0,t.jsx)(n.li,{children:"2: Novice, need significant help"}),"\n",(0,t.jsx)(n.li,{children:"1: Beginner, no understanding yet"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This comprehensive assessment framework ensures that students have mastered the key concepts and practical skills required for physical AI and humanoid robotics, with clear validation of learning outcomes throughout the course."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>r});var i=s(6540);const t={},a=i.createContext(t);function o(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);