"use strict";(globalThis.webpackChunkq4_hackathon=globalThis.webpackChunkq4_hackathon||[]).push([[7051],{3583:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"glossary","title":"Glossary of Terms","description":"Comprehensive glossary of technical terms used throughout the Physical AI & Humanoid Robotics Book","source":"@site/docs/glossary.md","sourceDirName":".","slug":"/glossary","permalink":"/Q4-hackathon1/docs/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/iffatmumtaz/Q4-hackathon1/edit/main/docs/glossary.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Glossary of Terms","description":"Comprehensive glossary of technical terms used throughout the Physical AI & Humanoid Robotics Book"}}');var t=i(4848),o=i(8453);const r={sidebar_position:1,title:"Glossary of Terms",description:"Comprehensive glossary of technical terms used throughout the Physical AI & Humanoid Robotics Book"},a="Glossary of Terms",l={},c=[{value:"A",id:"a",level:2},{value:"B",id:"b",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"E",id:"e",level:2},{value:"F",id:"f",level:2},{value:"G",id:"g",level:2},{value:"H",id:"h",level:2},{value:"I",id:"i",level:2},{value:"K",id:"k",level:2},{value:"L",id:"l",level:2},{value:"M",id:"m",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"T",id:"t",level:2},{value:"V",id:"v",level:2},{value:"W",id:"w",level:2},{value:"X",id:"x",level:2},{value:"Y",id:"y",level:2},{value:"Z",id:"z",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"glossary-of-terms",children:"Glossary of Terms"})}),"\n",(0,t.jsx)(n.p,{children:"This glossary defines key technical terms used throughout the Physical AI & Humanoid Robotics Book, organized alphabetically for easy reference."}),"\n",(0,t.jsx)(n.h2,{id:"a",children:"A"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Action Planning"}),": The process of determining a sequence of actions to achieve a specific goal, often involving task decomposition and resource allocation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"API (Application Programming Interface)"}),": A set of rules and protocols for building and interacting with software applications, allowing different systems to communicate."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Autonomous Robot"}),": A robot capable of performing tasks without human intervention, typically using sensors and AI to perceive and interact with its environment."]}),"\n",(0,t.jsx)(n.h2,{id:"b",children:"B"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Behavior Tree"}),": A hierarchical structure used in robotics and AI to organize and execute complex behaviors, providing a modular approach to task planning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bounding Box"}),": A rectangular box that encloses an object in 2D or 3D space, commonly used in computer vision for object detection."]}),"\n",(0,t.jsx)(n.h2,{id:"c",children:"C"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cloud Robotics"}),": A field that combines cloud computing with robotics, enabling robots to leverage cloud-based resources for computation, storage, and learning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command Parser"}),": A component that interprets natural language or structured commands and converts them into executable actions."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Computer Vision"}),": A field of artificial intelligence that enables computers to interpret and understand visual information from the world."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Conversational AI"}),": Artificial intelligence systems designed to engage in human-like conversations, often using natural language processing and machine learning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Coordinate Frame"}),": A system of reference axes used to define positions and orientations in space, essential for robotics and navigation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cross-Modal Learning"}),": Machine learning that combines information from multiple sensory modalities (e.g., vision, audio, touch) to improve understanding and performance."]}),"\n",(0,t.jsx)(n.h2,{id:"d",children:"D"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning"}),": A subset of machine learning that uses neural networks with multiple layers to model complex patterns in data."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Digital Twin"}),": A virtual replica of a physical object, system, or process that can be used for simulation, analysis, and optimization."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": A technique in robotics simulation where environment parameters are randomly varied to improve the transfer of learned behaviors from simulation to reality."]}),"\n",(0,t.jsx)(n.h2,{id:"e",children:"E"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Embodied AI"}),": Artificial intelligence that is integrated into physical systems, allowing AI agents to interact with the real world through robotic bodies."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Embodiment"}),": The concept of AI systems having a physical form that interacts with the environment, as opposed to purely virtual AI."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"End Effector"}),": The device at the end of a robotic arm designed to interact with the environment, such as a gripper or tool."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Environment Mapping"}),": The process of creating a representation of the robot's surroundings, typically using sensor data."]}),"\n",(0,t.jsx)(n.h2,{id:"f",children:"F"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Few-Shot Learning"}),": A machine learning approach where models learn to perform tasks with very limited training examples."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Forward Kinematics"}),": The process of calculating the position and orientation of a robot's end effector based on the joint angles."]}),"\n",(0,t.jsx)(n.h2,{id:"g",children:"G"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Generative Adversarial Networks (GANs)"}),": A class of machine learning frameworks where two neural networks compete to generate new data."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Geometric Reasoning"}),": The ability to understand and manipulate spatial relationships and geometric properties."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Gripper"}),": A robotic device designed to grasp and manipulate objects, analogous to a human hand."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grounding"}),": The process of connecting abstract symbols or concepts to sensory experiences or real-world referents."]}),"\n",(0,t.jsx)(n.h2,{id:"h",children:"H"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Human-Robot Interaction (HRI)"}),": The study of interactions between humans and robots, focusing on design, development, and evaluation of robotic systems for human use."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hybrid Planning"}),": An approach that combines symbolic (high-level) and geometric (low-level) planning methods."]}),"\n",(0,t.jsx)(n.h2,{id:"i",children:"I"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Inverse Kinematics"}),": The process of calculating the joint angles required to achieve a desired position and orientation of a robot's end effector."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"IoT (Internet of Things)"}),": A network of interconnected devices that can collect and exchange data over the internet."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Iterative Learning"}),": A process where systems improve their performance through repeated execution and feedback."]}),"\n",(0,t.jsx)(n.h2,{id:"k",children:"K"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Kinematics"}),": The study of motion without considering the forces that cause it, particularly relevant to robotic movement."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Knowledge Graph"}),": A structured representation of knowledge that describes relationships between entities."]}),"\n",(0,t.jsx)(n.h2,{id:"l",children:"L"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Large Language Model (LLM)"}),": Advanced AI models trained on vast amounts of text data, capable of understanding and generating human-like text."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Latent Space"}),": A lower-dimensional space in machine learning where complex data is represented in a compressed form."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning from Demonstration"}),": A method where robots learn tasks by observing and imitating human demonstrations."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"}),": A sensing technology that uses laser light to measure distances and create 3D maps."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Localization"}),": The process of determining a robot's position and orientation within a known map or environment."]}),"\n",(0,t.jsx)(n.h2,{id:"m",children:"M"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Manipulation"}),": The ability of a robot to physically interact with objects in its environment, typically through grasping and moving objects."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Mapping"}),": The process of creating a representation of the environment, often used in conjunction with localization (SLAM)."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Motion Planning"}),": The computational problem of finding a valid path for a robot to move from a start to a goal state while avoiding obstacles."]}),"\n",(0,t.jsx)(n.h2,{id:"n",children:"N"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Processing (NLP)"}),": A field of AI focused on enabling computers to understand, interpret, and generate human language."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"NeRF (Neural Radiance Fields)"}),": A technique for synthesizing novel views of complex 3D scenes using neural networks."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Neural-Symbolic Integration"}),": Approaches that combine neural networks with symbolic reasoning systems."]}),"\n",(0,t.jsx)(n.h2,{id:"o",children:"O"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": The computer vision task of identifying and locating objects within images or video streams."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Ontology"}),": A formal representation of knowledge as a set of concepts within a domain and the relationships between those concepts."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Open-Source Robotics"}),": Robotics software and hardware that is freely available for use, modification, and distribution."]}),"\n",(0,t.jsx)(n.h2,{id:"p",children:"P"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Path Planning"}),": The process of determining a geometric path for a robot to follow from a start to a goal position."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": The ability of a robot to interpret sensory information from its environment."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Physical AI"}),": The integration of artificial intelligence with physical systems, enabling robots to interact intelligently with the real world."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Point Cloud"}),": A set of data points in space, typically representing the external surfaces of objects, commonly used in 3D mapping."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prompt Engineering"}),": The practice of designing effective prompts to guide the behavior of large language models."]}),"\n",(0,t.jsx)(n.h2,{id:"r",children:"R"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reactive System"}),": A system that responds to changes in its environment without maintaining an internal state model."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning"}),": A type of machine learning where agents learn to make decisions by receiving rewards or penalties."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robot Operating System (ROS)"}),": Flexible framework for writing robot software, providing services designed for heterogeneous computer clusters."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robotics Middleware"}),": Software that provides common services and capabilities for robot applications, facilitating communication between components."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ROS 2"}),": The second generation of the Robot Operating System, featuring improved architecture and real-time capabilities."]}),"\n",(0,t.jsx)(n.h2,{id:"s",children:"S"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scene Understanding"}),": The process of interpreting and comprehending the content and context of visual scenes."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Semantic Segmentation"}),": A computer vision task that assigns a semantic label to each pixel in an image."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simulation-to-Reality Transfer"}),": The process of transferring behaviors or policies learned in simulation to real-world robotic systems."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Spatial Reasoning"}),": The cognitive process of understanding and manipulating spatial relationships between objects."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"State Estimation"}),": The process of determining the current state of a system based on noisy sensor measurements."]}),"\n",(0,t.jsx)(n.h2,{id:"t",children:"T"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task Planning"}),": The process of determining a sequence of actions to achieve a specific goal, considering constraints and resources."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Teleoperation"}),": The remote control of a robot by a human operator, often used for tasks in hazardous environments."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Transform"}),": In robotics, a mathematical operation that describes the relationship between different coordinate frames."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Trust Calibration"}),": The process of ensuring that users have appropriate levels of trust in autonomous systems."]}),"\n",(0,t.jsx)(n.h2,{id:"v",children:"V"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Variational Autoencoder (VAE)"}),": A generative model that learns to encode data into a latent space and decode it back."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": Systems that integrate visual perception, language understanding, and physical action for robotic control."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visual Servoing"}),": The use of visual feedback to control robot motion."]}),"\n",(0,t.jsx)(n.h2,{id:"w",children:"W"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"World Model"}),": An internal representation that an agent maintains of its environment, used for planning and decision-making."]}),"\n",(0,t.jsx)(n.h2,{id:"x",children:"X"}),"\n",(0,t.jsx)(n.p,{children:"(Xenial term definitions would go here if applicable to the domain)"}),"\n",(0,t.jsx)(n.h2,{id:"y",children:"Y"}),"\n",(0,t.jsx)(n.p,{children:"(Yet more specialized terms would be defined here)"}),"\n",(0,t.jsx)(n.h2,{id:"z",children:"Z"}),"\n",(0,t.jsx)(n.p,{children:"(Zettabyte-scale data processing for robotics applications, though this is speculative for the domain)"}),"\n",(0,t.jsx)(n.p,{children:"This glossary serves as a reference for technical terms used throughout the Physical AI & Humanoid Robotics Book, supporting readers in understanding the specialized vocabulary of the field."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);